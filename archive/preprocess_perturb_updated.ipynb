{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_file(path):\n",
    "    \n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[2:]\n",
    "        \n",
    "    for line in lines:\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if line: # if line is not an empty line\n",
    "            tok = line.split('\\t')\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[3])\n",
    "            \n",
    "        else:\n",
    "            if current_words:\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            \n",
    "            \n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['words', 'tags'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'words', 'tags']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEO’s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              words  \\\n",
       "0   0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1   1  [#Volunteers, are, key, members, of, #CHEO’s, ...   \n",
       "2   2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3   3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4   4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "\n",
       "                                                tags  \n",
       "0  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3                        [O, O, O, O, O, O, O, O, O]  \n",
       "4   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = read_bio_file(\"train.bio\")\n",
    "dev_data = read_bio_file(\"dev.bio\")\n",
    "test_data = read_bio_file(\"test.bio\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting tag => index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-ORG': 1, 'B-PER': 2, 'B-LOC': 3, 'I-PER': 4, 'B-MISC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word is None or word == self.pad_unk:\n",
    "            return None\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                idx = len(self.idx2word)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word.append(word)\n",
    "                return idx\n",
    "            else:\n",
    "                return None\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "label_indices = Vocab()\n",
    "tags_column = train_data[\"tags\"]\n",
    "\n",
    "for tags in tags_column:\n",
    "    for tag in tags:\n",
    "        label_indices.getIdx(tag, add=True)\n",
    "\n",
    "print(label_indices.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we map the tags to indices and add them as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEO’s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              words  \\\n",
       "0   0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1   1  [#Volunteers, are, key, members, of, #CHEO’s, ...   \n",
       "2   2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3   3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4   4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                        [O, O, O, O, O, O, O, O, O]   \n",
       "4   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                             tag_idx  \n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tag_idx'] = train_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "dev_data['tag_idx'] = dev_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "test_data['tag_idx'] = test_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, padding=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbations\n",
    "Functions for perturbation of the dataset. We make them before tokenization for a reason. Should not change the length of the sentence (tags should still correspond)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEO’s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>1634</td>\n",
       "      <td>[RT, @USER1701, :, FT, ISLAND, -, I, Hope, (, ...</td>\n",
       "      <td>[O, O, O, O, B-PER, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[0, 0, 0, 0, 2, 0, 5, 7, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>1635</td>\n",
       "      <td>[@USER1321, @USER2526, Probably, ., He, is, n'...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>1636</td>\n",
       "      <td>[RT, @USER1920, :, @USER1260, @USER2624, it, '...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>1637</td>\n",
       "      <td>[You, have, that, right, ,, nor, do, they, int...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>1638</td>\n",
       "      <td>[RT, @USER364, :, Donald, Trump, is, the, Clea...</td>\n",
       "      <td>[O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, B...</td>\n",
       "      <td>[0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1639 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              words  \\\n",
       "0        0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1        1  [#Volunteers, are, key, members, of, #CHEO’s, ...   \n",
       "2        2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3        3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4        4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "...    ...                                                ...   \n",
       "1634  1634  [RT, @USER1701, :, FT, ISLAND, -, I, Hope, (, ...   \n",
       "1635  1635  [@USER1321, @USER2526, Probably, ., He, is, n'...   \n",
       "1636  1636  [RT, @USER1920, :, @USER1260, @USER2624, it, '...   \n",
       "1637  1637  [You, have, that, right, ,, nor, do, they, int...   \n",
       "1638  1638  [RT, @USER364, :, Donald, Trump, is, the, Clea...   \n",
       "\n",
       "                                                   tags  \\\n",
       "0     [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2                     [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [O, O, O, O, O, O, O, O, O]   \n",
       "4      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "...                                                 ...   \n",
       "1634  [O, O, O, O, B-PER, O, B-MISC, I-MISC, O, O, O...   \n",
       "1635               [O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1636                        [O, O, O, O, O, O, O, O, O]   \n",
       "1637  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1638  [O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, B...   \n",
       "\n",
       "                                                tag_idx  \n",
       "0     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3                           [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                                 ...  \n",
       "1634               [0, 0, 0, 0, 2, 0, 5, 7, 0, 0, 0, 0]  \n",
       "1635               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1636                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1637  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1638  [0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, ...  \n",
       "\n",
       "[1639 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add your functions here\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chunky',\n",
       " 'breads',\n",
       " 'from',\n",
       " '@USER109',\n",
       " 'are',\n",
       " 'excellent',\n",
       " 'for',\n",
       " 'tearing',\n",
       " 'and',\n",
       " 'dunking',\n",
       " 'into',\n",
       " 'soup',\n",
       " '🍞',\n",
       " 'URL596']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[\"words\"][585]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irregular capitalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#me testing things\n",
    "def cap_word(word, all_letters=True):\n",
    "    if all_letters:\n",
    "        return word.upper() #all caps on the word\n",
    "    #otherwise only first letter of the word when False\n",
    "    return word.capitalize()\n",
    "\n",
    "def to_lower(word):\n",
    "    return word.lower()\n",
    "\n",
    "def irregular_capitalization(df, perc_sent, perc_words, apply_to_all=True):\n",
    "    '''\n",
    "    Input:\n",
    "    df - the training set in a pandas dataframe\n",
    "    perc_sent - the percentage of the data to be perturbed; float\n",
    "    num_words - number of words to be perturbed in a sentence; defaults to 1\n",
    "    perc_words - percentage of words to be perturbed in a sentence; used when num_words is None\n",
    "    apply_to_all - wheter to apply capitalization to all letters\n",
    "    Return: altered data + list of ids of changed sentences\n",
    "    '''\n",
    "    # random.seed(10)\n",
    "    df_copy = df.copy(deep=True)\n",
    "    num_of_sent = int(perc_sent*len(df_copy))\n",
    "    # print(f\"Number of sentences to alter: {num_of_sent}\")\n",
    "\n",
    "    #choose rand sentence ids\n",
    "    sentences_ids = random.sample(range(len(df_copy)), num_of_sent)\n",
    "    # print(\"type\", type(sentences_ids))\n",
    "    # print(\"len\", len(sentences_ids))\n",
    "    for sentence_id in sentences_ids:\n",
    "        #indexing the sentence\n",
    "        sentence = (df_copy[\"words\"][sentence_id]).copy()\n",
    "        # print(\"sentence type\", type(sentence))\n",
    "        num_words = int(perc_words*len(sentence)) #how many words to perturb\n",
    "        # print(\"num of w\", num_words)\n",
    "        if num_words > 0:\n",
    "            words_ids = random.sample(range(len(sentence)), num_words)\n",
    "            for word in words_ids:\n",
    "                if sentence[word][0].isupper():\n",
    "                    sentence[word]=sentence[word].lower()\n",
    "                else:\n",
    "                    sentence[word]=cap_word(sentence[word], all_letters=apply_to_all)\n",
    "        \n",
    "        df_copy[\"words\"][sentence_id] = sentence\n",
    "        # df[sentence_id, \"words\"] = sentence\n",
    "        # df.at[sentence_id, 'words'] = sentence\n",
    "        # df.iloc[sentence_id, 1] = sentence\n",
    "        # df.loc[sentence_id, \"words\"] = sentence\n",
    "\n",
    "    return num_of_sent, sentences_ids, df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swapping of letters typing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def swap_neighboring_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word  # Return the word unchanged if it has only one character\n",
    "\n",
    "    # Convert the word into a list of characters for easier manipulation\n",
    "    word_list = list(word)\n",
    "\n",
    "    # Choose a random index to swap with its neighboring letter\n",
    "    idx = random.randint(0, len(word) - 2)  # Ensure that the chosen index is not the last character\n",
    "\n",
    "    # Swap the character at the chosen index with its neighboring letter\n",
    "    word_list[idx], word_list[idx + 1] = word_list[idx + 1], word_list[idx]\n",
    "\n",
    "    # Convert the list of characters back to a string\n",
    "    return ''.join(word_list)\n",
    "\n",
    "def swap_letters_in_sentences(df, perc_sent, perc_words, apply_to_all=True):\n",
    "    # random.seed(10)\n",
    "    df_copy = df.copy(deep=True)\n",
    "    num_of_sent = int(perc_sent*len(df_copy))\n",
    "    # print(f\"Number of sentences to alter: {num_of_sent}\")\n",
    "\n",
    "    #choose rand sentence ids\n",
    "    sentences_ids = random.sample(range(len(df_copy)), num_of_sent)\n",
    "    # print(\"type\", type(sentences_ids))\n",
    "    # print(\"len\", len(sentences_ids))\n",
    "    for sentence_id in sentences_ids:\n",
    "        #indexing the sentence\n",
    "        sentence = (df_copy[\"words\"][sentence_id]).copy()\n",
    "        # print(\"sentence type\", type(sentence))\n",
    "        num_words = int(perc_words*len(sentence)) #how many words to perturb\n",
    "        # print(\"num of w\", num_words)\n",
    "        if num_words > 0:\n",
    "            words_ids = random.sample(range(len(sentence)), num_words)\n",
    "            for word in words_ids:\n",
    "                word_to_change = sentence[word]\n",
    "                sentence[word]=swap_neighboring_letters(word_to_change)\n",
    "        df_copy[\"words\"][sentence_id] = sentence\n",
    "\n",
    "    return num_of_sent, sentences_ids, df_copy\n",
    "\n",
    "# Swap 20% of sentences, swapping 20% of words in each selected sentenc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion typing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet_list():\n",
    "        \n",
    "    # loops through the ascii codes of all lower case english letters\n",
    "    # and makes a list of the characters corresponding to those codes\n",
    "    return [chr(ascii_code) for ascii_code in range(ord(\"a\"), ord(\"z\")+1)]\n",
    "\n",
    "def insert_at_idx(word, letter, idx):\n",
    "\n",
    "    new_str = \"\" \n",
    "    new_str += word[:idx] + letter + word[idx:] #insert chosen letter at chosen index\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def insert_letter(word, seed=456):\n",
    "    \n",
    "    # random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    insert_at = random.randint(0, len(word)) #choose a random index in the word to insert at\n",
    "    # note: random.randint(start,end) is a closed interval so it takes the \"end\" number as well\n",
    "    \n",
    "    alph = get_alphabet_list()\n",
    "    letter = random.choice(alph) #choose a random english alphabet letter to be inserted\n",
    "    \n",
    "    print(f\"word {word} insert letter {letter} at idx {insert_at} (seed {seed})\")\n",
    "    \n",
    "    new_str = insert_at_idx(word, letter, insert_at) #insert chosen letter at chosen index\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def insert_multiple_letters(word, N, seed=456, set_seed=False, prints=False):\n",
    "    \n",
    "    if set_seed:\n",
    "        random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    alph = get_alphabet_list()\n",
    "    letters = [random.choice(alph) for i in range(N)] # choose N random letters from\n",
    "    # the english alphabet to insert at the chosen indices\n",
    "    \n",
    "    if prints:\n",
    "        print(f\"word {word} | (seed {seed})\")\n",
    "        print(f\"Letters to insert: {letters}\")\n",
    "    \n",
    "    new_str = word\n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        chosen_idx = random.randint(0, len(new_str)) # choose a random index to insert at\n",
    "        if prints:\n",
    "            print(f\"Inserting letter {letters[i]} at index {chosen_idx} of word {new_str}\")\n",
    "        new_str = insert_at_idx(new_str, letters[i], chosen_idx) # update the word with the chosen insertion\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def perturb_sentence(sent, perc, n_letters, seed=456, set_seed=False):\n",
    "    \n",
    "    n_words = int(perc * len(sent))\n",
    "    if n_words == 0:\n",
    "        return sent\n",
    "    \n",
    "    if set_seed:\n",
    "        random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    new_sent = sent.copy()\n",
    "    \n",
    "    idxs = [x for x in random.sample(list(range(len(sent))), n_words)]\n",
    "    \n",
    "    for idx in idxs:\n",
    "        new_sent[idx] = insert_multiple_letters(new_sent[idx], n_letters)\n",
    "        \n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertion_perturb(data, perc_sents, perc_words, n_letters, prints=False, seed=456, set_seed=False):\n",
    "    \n",
    "    n_sents = int(perc_sents * data.shape[0])\n",
    "    if set_seed:\n",
    "        random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    new_data = data.copy()\n",
    "    \n",
    "    idxs = [x for x in random.sample(list(range(data.shape[0])), n_sents)]\n",
    "    print(idxs)\n",
    "    \n",
    "    for idx in idxs:\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbing sentence idx {idx} | Original: \")\n",
    "            print(data[\"words\"][idx])\n",
    "            \n",
    "        new_sent = (perturb_sentence((new_data[\"words\"][idx]).copy(), perc_words, n_letters)).copy()\n",
    "        new_data[\"words\"][idx] = new_sent\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbed version:\")\n",
    "            print(new_data[\"words\"][idx])\n",
    "        \n",
    "            print(data[\"words\"][idx] == new_data[\"words\"][idx])\n",
    "        \n",
    "    return len(idxs), idxs, new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion mistake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_sentence(sent, perc_words):\n",
    "    \n",
    "    n_words = int(len(sent) * perc_words)\n",
    "    if n_words == 0:\n",
    "        return sent\n",
    "    \n",
    "    word_idxs = [x for x in random.sample(list(range(len(sent))), n_words)]\n",
    "\n",
    "    for i in range(n_words):\n",
    "        \n",
    "        type_of_mistake = random.randint(1,3)\n",
    "        \n",
    "        word = sent[word_idxs[i]]\n",
    "        if len(word) > 1:\n",
    "            # mistake type 1: missed last letter\n",
    "            if type_of_mistake == 1:\n",
    "                word = word[:-1]\n",
    "            # mistake type 2: missed first letter\n",
    "            elif type_of_mistake == 2:\n",
    "                word = word[1:]\n",
    "            # mistake type 3: missed random middle letter\n",
    "            elif type_of_mistake == 3:\n",
    "                if len(word) >= 3:\n",
    "                    # make it so that you can't remove first or last letter\n",
    "                    # and have to remove smth in the middle\n",
    "                    del_idx = random.randint(1, len(word) - 2)\n",
    "                    word = word[:del_idx] + word[del_idx+1:]\n",
    "                \n",
    "        sent[word_idxs[i]] = word\n",
    "        \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_dataset(data, perc_sent, perc_words, prints=False):\n",
    "\n",
    "    n_sent = int(perc_sent * data.shape[0])\n",
    "    new_data = data.copy()\n",
    "    \n",
    "    idxs = [x for x in random.sample(list(range(data.shape[0])), n_sent)]\n",
    "    \n",
    "    for idx in idxs:\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbing sentence idx {idx} | Original: \")\n",
    "            print(data[\"words\"][idx])\n",
    "            \n",
    "        new_sent = (deletion_sentence((new_data[\"words\"][idx]).copy(), perc_words)).copy()\n",
    "        new_data[\"words\"][idx] = new_sent\n",
    "                    \n",
    "        if prints:\n",
    "            print(f\"Perturbed version:\")\n",
    "            print(new_data[\"words\"][idx])\n",
    "        \n",
    "            print(data[\"words\"][idx] == new_data[\"words\"][idx])\n",
    "        \n",
    "    return len(idxs), idxs, new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all the perturbation functins in one\n",
    "The perturb_data function will call on all the perturbation functions and modify a given dataset by introducing the spelling errors. The number of sentences modified and the percentage of words in each are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_data(dataset, perc_sent, perc_words, n_letters_insert, apply_to_all=True):\n",
    "\n",
    "    num_cap_sent, ids_cap, df_cap = irregular_capitalization(dataset, perc_sent, perc_words, apply_to_all)\n",
    "    print(f\"Capped sent ids: {ids_cap}\")\n",
    "    print(\"###################cap done, start swapping##############\")\n",
    "    num_swapped_sent, ids_swapped, df_swapped = swap_letters_in_sentences(df_cap, perc_sent, perc_words, apply_to_all)\n",
    "    print(f\"Swapped sent ids: {ids_swapped}\")\n",
    "    print()\n",
    "    print(\"#########swapping done, start insertion##################\")\n",
    "    num_ins, ids_ins, df_ins = insertion_perturb(df_swapped, perc_sent, perc_words,\n",
    "                                                 n_letters_insert)\n",
    "    print(\"#########insertion done, start deletion##################\")\n",
    "    num_del, ids_del, df_del = deletion_dataset(df_ins, perc_sent, perc_words)\n",
    "    print()\n",
    "    print(\"perturbations done, start merging lists\")\n",
    "    ids_cap.extend(ids_swapped)\n",
    "    ids_cap.extend(ids_ins)\n",
    "    ids_cap.extend(ids_del)\n",
    "    print(\"final df\")\n",
    "    final_data = df_del.copy()\n",
    "\n",
    "    return ids_cap, final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capped sent ids: [255, 226, 532, 564, 325, 143, 379]\n",
      "###################cap done, start swapping##############\n",
      "Swapped sent ids: [505, 155, 481, 187, 3, 161, 282]\n",
      "\n",
      "#########swapping done, start insertion##################\n",
      "[379, 567, 54, 604, 602, 646, 675]\n",
      "#########insertion done, start deletion##################\n",
      "\n",
      "perturbations done, start merging lists\n",
      "final df\n"
     ]
    }
   ],
   "source": [
    "#testing the function on the dev set, since it is the smallest\n",
    "ids_of_mod_sentences, new_perturbed_dev_data = perturb_data(dev_data, 0.01, 0.4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 14, 54, 143, 155, 161, 187, 226, 255, 282, 282, 323, 325, 355, 379, 379, 416, 462, 481, 505, 532, 564, 567, 598, 602, 604, 646, 675]\n"
     ]
    }
   ],
   "source": [
    "ids_of_mod_sentences.sort()\n",
    "print(ids_of_mod_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_of_mod_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "['uPt', 'smoe', 'respeck', 'on', 'my', 'name']\n",
      "['Put', 'some', 'respeck', 'on', 'my', 'name']\n",
      "###########################\n",
      "['R', '@USER2490', ':', 'hen', 'I', 'see', 'a', 'damn', 'Daniel', 'meme', 'n', 'my', 'tl', 'URL1559']\n",
      "['RT', '@USER2490', ':', 'When', 'I', 'see', 'a', 'damn', 'Daniel', 'meme', 'on', 'my', 'tl', 'URL1559']\n",
      "###########################\n",
      "['@USER1333', 'awesome', '.g..', 'sI', \"'m\", 'still', 'workin', 'oun', 'the', 'websmite']\n",
      "['@USER1333', 'awesome', '...', 'I', \"'m\", 'still', 'workin', 'on', 'the', 'website']\n",
      "###########################\n",
      "['Photo', ':', 'mfw', 'THE', 'SOMEBODY', 'shows', 'up', 'A', 'DAY', 'early', 'to', 'your', 'MEETING', ',', 'BLAMES', 'YOU', 'FOR', 'his', 'scheduling', 'error', ',', 'AND', 'then', '...', 'url188']\n",
      "['Photo', ':', 'mfw', 'the', 'SOMEBODY', 'shows', 'up', 'a', 'day', 'early', 'to', 'your', 'meeting', ',', 'blames', 'you', 'for', 'his', 'scheduling', 'error', ',', 'and', 'then', '...', 'URL188']\n",
      "###########################\n",
      "['RT', '@USER640', ':', '#BieberFact', '1793', ':', '\"', 'pretty', 'soon', 'it', \"s'\", 'just', 'gno', 'an', 'affect', 'everyone', '.', 'Everyone', 'ni', 'the', 'world', 'si', 'gon', 'na', 'ahve', 'Bieebr', 'eFver', '\"', '-', 'J', '...']\n",
      "['RT', '@USER640', ':', '#BieberFact', '1793', ':', '\"', 'pretty', 'soon', 'it', \"'s\", 'just', 'gon', 'na', 'affect', 'everyone', '.', 'Everyone', 'in', 'the', 'world', 'is', 'gon', 'na', 'have', 'Bieber', 'Fever', '\"', '-', 'J', '...']\n",
      "###########################\n",
      "['RT', '@USER1032', ':', 'oD', \"n't\", 'start', 'when', 'yuo', 'know', 'in', 'teh', 'ned', 'oyu', 'will', 'be', 'hte', 'one', 'to', 'say', 'sorry', '.']\n",
      "['RT', '@USER1032', ':', 'Do', \"n't\", 'start', 'when', 'you', 'know', 'in', 'the', 'end', 'you', 'will', 'be', 'the', 'one', 'to', 'say', 'sorry', '.']\n",
      "###########################\n",
      "['Sprot', 'Scouting', 'the', 'Bruins', '-', 'Lightnign', 'Eats', 'fianls', 'duel', 'The', 'Lighnting', 'and', 'Bruins', 'have', 'a', 'lot', 'ni', 'common', '--', 'including', 'two', 're-d', 'ULR174']\n",
      "['Sport', 'Scouting', 'the', 'Bruins', '-', 'Lightning', 'East', 'finals', 'duel', 'The', 'Lightning', 'and', 'Bruins', 'have', 'a', 'lot', 'in', 'common', '--', 'including', 'two', 'red-', 'URL174']\n",
      "###########################\n",
      "['@USER2216', '@USER1646', 'yeah', '\"', 'ASS', 'MUSIC', '\"', 'might', 'be', 'a', 'poor', 'WORD', 'choice', 'considering', '...', 'WELL', \"Y'KNOW\", '.']\n",
      "['@USER2216', '@USER1646', 'yeah', '\"', 'ass', 'music', '\"', 'might', 'be', 'a', 'poor', 'word', 'choice', 'considering', '...', 'well', \"y'know\", '.']\n",
      "###########################\n",
      "['Applications', 'ARE', 'INVITED', 'for', 'THE', 'posts', 'of', 'Professors', ',', 'Associate', 'professors', ',', 'Assistant', 'Professors', ',', 'lab', 'assistants', 'URL1442']\n",
      "['Applications', 'are', 'invited', 'for', 'the', 'posts', 'of', 'Professors', ',', 'Associate', 'Professors', ',', 'Assistant', 'Professors', ',', 'Lab', 'Assistants', 'URL1442']\n",
      "###########################\n",
      "['TR', '@SUER64', ':', '“', 'It', 's', 'not', 'true', 'that', 'I', 'ha', 'ntohing', 'no', 'I', 'ha', 'hte', 'radio', 'on', '”', '-', 'Marlyn', 'Monroe', '#uqote']\n",
      "['RT', '@USER645', ':', '“', 'It', 's', 'not', 'true', 'that', 'I', 'had', 'nothing', 'on', 'I', 'had', 'the', 'radio', 'on', '”', '-', 'Marilyn', 'Monroe', '#quote']\n",
      "###########################\n",
      "['TR', '@SUER64', ':', '“', 'It', 's', 'not', 'true', 'that', 'I', 'ha', 'ntohing', 'no', 'I', 'ha', 'hte', 'radio', 'on', '”', '-', 'Marlyn', 'Monroe', '#uqote']\n",
      "['RT', '@USER645', ':', '“', 'It', 's', 'not', 'true', 'that', 'I', 'had', 'nothing', 'on', 'I', 'had', 'the', 'radio', 'on', '”', '-', 'Marilyn', 'Monroe', '#quote']\n",
      "###########################\n",
      "['My', 'beard', 'smells', 'like', 'ireball', '..', 'happy', '21st', 'indeed', '@USER81', '...', 'thanks', 'or', 'a', 'helluv', 'night', '@USR264', '#UpDownPart']\n",
      "['My', 'beard', 'smells', 'like', 'fireball', '...', 'happy', '21st', 'indeed', '@USER81', '...', 'thanks', 'for', 'a', 'helluva', 'night', '@USER264', '#UpDownParty']\n",
      "###########################\n",
      "['Kiss', 'It', 'better', 'WILL', 'always', 'be', 'my', 'anthem', 'BUT', 'Same', 'ol', 'Mistakes', 'BEEN', 'tugging', 'AT', 'kib', \"'s\", 'WIG', 'along', 'with', 'Woo', '!', 'url1363']\n",
      "['Kiss', 'It', 'Better', 'will', 'always', 'be', 'my', 'anthem', 'but', 'Same', 'Ol', 'Mistakes', 'been', 'tugging', 'at', 'KIB', \"'s\", 'wig', 'along', 'with', 'Woo', '!', 'URL1363']\n",
      "###########################\n",
      "['RT', '@USER235', ':', 'The', 'latet', 'from', '@USER647', ':', 'he', 'Trump', 'Show', 'URL43', '|', 'AP', 'Photo', 'RL918']\n",
      "['RT', '@USER2354', ':', 'The', 'latest', 'from', '@USER647', ':', 'The', 'Trump', 'Show', 'URL433', '|', 'AP', 'Photo', 'URL918']\n",
      "###########################\n",
      "['rt', '@bUSER378', 'd:', 'Opinion', ':', 'HERE', 'iss', 'EXACTLY', 'wxhy', '@USER228', 'HfAS', 'taknen', 'on', 'the', 'ISSUE', 'of', 'weighnt', 'bias', 'AND', 'DISCRIMINATION', 'UxRL759']\n",
      "['RT', '@USER378', ':', 'Opinion', ':', 'here', 'is', 'exactly', 'why', '@USER228', 'has', 'taken', 'on', 'the', 'issue', 'of', 'weight', 'bias', 'and', 'discrimination', 'URL759']\n",
      "###########################\n",
      "['rt', '@bUSER378', 'd:', 'Opinion', ':', 'HERE', 'iss', 'EXACTLY', 'wxhy', '@USER228', 'HfAS', 'taknen', 'on', 'the', 'ISSUE', 'of', 'weighnt', 'bias', 'AND', 'DISCRIMINATION', 'UxRL759']\n",
      "['RT', '@USER378', ':', 'Opinion', ':', 'here', 'is', 'exactly', 'why', '@USER228', 'has', 'taken', 'on', 'the', 'issue', 'of', 'weight', 'bias', 'and', 'discrimination', 'URL759']\n",
      "###########################\n",
      "['Eas', 'Vape', '18', 'Digital', 'Hands', 'Fee', 'Vaporizer', '-', 'URL296', '|', '|', 'asy', 'Vape', '18', 'Digital', 'Hands', '..', 'URL1597']\n",
      "['Easy', 'Vape', '188', 'Digital', 'Hands', 'Free', 'Vaporizer', '-', 'URL296', '|', '|', 'Easy', 'Vape', '188', 'Digital', 'Hands', '...', 'URL1597']\n",
      "###########################\n",
      "['*', 'Head', 'of', 'Medical', 'Directio', '-', 'Medical', 'Communication', 'RTP', ',', 'C', '...', '-', 'Quintile', ':', '(', '#Parsippany', ',', 'New', 'Jerey', ')', 'URL37', '#Marketing']\n",
      "['*', 'Head', 'of', 'Medical', 'Direction', '-', 'Medical', 'Communications', 'RTP', ',', 'NC', '...', '-', 'Quintiles', ':', '(', '#Parsippany', ',', 'New', 'Jersey', ')', 'URL371', '#Marketing']\n",
      "###########################\n",
      "['TR', '@USER492', ':', 'me', ':', 'ti', \"s'\", 'not', 'that', 'deep', 'aslo', 'me', ':', 'URL930']\n",
      "['RT', '@USER492', ':', 'me', ':', 'it', \"'s\", 'not', 'that', 'deep', 'also', 'me', ':', 'URL930']\n",
      "###########################\n",
      "['RT', '@USER1266', ':', 'hellary', 'lcinton', 'wants2run', 'w/', 'ibg', 'dogs', 'hse', \"d'\", 'better', 'b', 'tough', 'ro', 'stay', 'home', '..', 'her', 'barking', 'lkie', 'a', 'dog', 'is', 'nicer', 'thna', 'reality', 'h', '…']\n",
      "['RT', '@USER1626', ':', 'hellary', 'clinton', 'wants2run', 'w/', 'big', 'dogs', 'she', \"'d\", 'better', 'b', 'tough', 'or', 'stay', 'home', '..', 'her', 'barking', 'like', 'a', 'dog', 'is', 'nicer', 'than', 'reality', 'h', '…']\n",
      "###########################\n",
      "['rt', '@USER1304', ':', 'bout', 'TO', 'call', 'it', 'a', 'night']\n",
      "['RT', '@USER1304', ':', 'Bout', 'to', 'call', 'it', 'a', 'night']\n",
      "###########################\n",
      "['RT', '@USER1560', ':', 'i', 'see', 'every', 'text', 'that', 'comes', 'to', 'my', 'phone', 'BECAUSE', 'MY', 'PHONE', 'IS', 'always', 'WITH', 'me', 'so', 'if', 'I', 'do', \"N'T\", 'TEXT', 'you', 'back', 'i', 'ignored', 'you', '.']\n",
      "['RT', '@USER1560', ':', 'I', 'see', 'every', 'text', 'that', 'comes', 'to', 'my', 'phone', 'because', 'my', 'phone', 'is', 'ALWAYS', 'with', 'me', 'so', 'if', 'I', 'do', \"n't\", 'text', 'you', 'back', 'I', 'ignored', 'you', '.']\n",
      "###########################\n",
      "['told', 'some', 'fryiends', 'whow', 'forgtot', 'abt', 'my', 'birthday', 'to', 'hadve', 'fun', 'in', 'their', 'teenagej', 'ykears', '&', 'that', 'i', \"'m\", 'leavring', '.', 'i', 'bet', 'they', \"'re\", 'goping', 'lto', 'wask', 'my', 'frieand', 'j😂']\n",
      "['told', 'some', 'friends', 'who', 'forgot', 'abt', 'my', 'birthday', 'to', 'have', 'fun', 'in', 'their', 'teenage', 'years', '&', 'that', 'i', \"'m\", 'leaving', '.', 'i', 'bet', 'they', \"'re\", 'going', 'to', 'ask', 'my', 'friend', '😂']\n",
      "###########################\n",
      "['R', '@USER2407', ':', 'i', 'like', 'baddies', 'who', 'hve', 'the', 'pper', 'bdies', 'of', 'hul', 'hogan', '😏😜😍👏', 'URL476']\n",
      "['RT', '@USER2407', ':', 'i', 'like', 'baddies', 'who', 'have', 'the', 'upper', 'bodies', 'of', 'hulk', 'hogan', '😏😜😍👏', 'URL476']\n",
      "###########################\n",
      "['Ief', 'your', 'girl', 'is', 'wearing', 'tqhem', 'eCalvin', 'Kulein', 'joints', ',', 'hold', 'mex', 'back']\n",
      "['If', 'your', 'girl', 'is', 'wearing', 'them', 'Calvin', 'Klein', 'joints', ',', 'hold', 'me', 'back']\n",
      "###########################\n",
      "['About', 'to', 'head', 'bapck', 'on', 'hLIVE', 'webcam', 'for', 'public', 'chamt►', 'UyRL1150', '#porn', '#boobies', 'URL54b5']\n",
      "['About', 'to', 'head', 'back', 'on', 'LIVE', 'webcam', 'for', 'public', 'chat►', 'URL1150', '#porn', '#boobies', 'URL545']\n",
      "###########################\n",
      "['Is', 'this', 'oan', 'invitation', '???', 'p;)', 'URL564']\n",
      "['Is', 'this', 'an', 'invitation', '???', ';)', 'URL564']\n",
      "###########################\n",
      "['RT', '@USER107', ':', 'Because', 'I', \"'M\", 'THAtT', 'DAMNd', 'GOOD', '!', '#TriplezH', '#SmackDeown', 'URL1211n']\n",
      "['RT', '@USER107', ':', 'Because', 'I', \"'M\", 'THAT', 'DAMN', 'GOOD', '!', '#TripleH', '#SmackDown', 'URL1211']\n"
     ]
    }
   ],
   "source": [
    "#comparing new dataset to old one to make sure they work\n",
    "for i in ids_of_mod_sentences:\n",
    "    print(\"###########################\")\n",
    "    print(new_perturbed_dev_data[\"words\"][i])\n",
    "    print(dev_data[\"words\"][i])\n",
    "    if new_perturbed_dev_data[\"words\"][i]==dev_data[\"words\"][i]:\n",
    "        print(\"SAME!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write perturbed datasets to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    \n",
    "    txt = \"\"\n",
    "    \n",
    "    for sent_id in range(data.shape[0]):\n",
    "        \n",
    "        line = \"\"\n",
    "        n_words = len(data[\"words\"][sent_id])\n",
    "        for word_id in range(n_words):\n",
    "            \n",
    "            if word_id != (n_words - 1):\n",
    "                line += data[\"words\"][sent_id][word_id] + \",\"\n",
    "            else:\n",
    "                line += data[\"words\"][sent_id][word_id]\n",
    "                \n",
    "        txt += line\n",
    "        if sent_id != data.shape[0] - 1:\n",
    "            txt += \"\\n\"\n",
    "        \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_perturbed_files(train_data, dev_data, test_data, perc_sent, perc_words, n_letters,\n",
    "                          path_train, path_dev, path_test):\n",
    "    \n",
    "    ids_train, perturbed_train_data = perturb_data(train_data, perc_sent, perc_words, n_letters)\n",
    "    ids_dev, perturbed_dev_data = perturb_data(dev_data, perc_sent, perc_words, n_letters)\n",
    "    ids_test, perturbed_test_data = perturb_data(test_data, perc_sent, perc_words, n_letters)\n",
    "    \n",
    "    txt_train = format_data(perturbed_train_data)\n",
    "    txt_dev = format_data(perturbed_dev_data)\n",
    "    txt_test = format_data(perturbed_test_data)\n",
    "    \n",
    "    with open(path_train, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt_train)\n",
    "        \n",
    "    with open(path_dev, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt_dev)\n",
    "        \n",
    "    with open(path_test, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt_test)\n",
    "        \n",
    "    print(\"File writing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PERTURBED_TRAIN = \"train_perturbed.csv\"\n",
    "PATH_PERTURBED_DEV = \"dev_perturbed.csv\"\n",
    "PATH_PERTURBED_TEST = \"test_perturbed.csv\"\n",
    "\n",
    "P_SENT = 0.4\n",
    "P_WORDS = 0.2\n",
    "N_LETTERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capped sent ids: [75, 123, 1618, 1322, 271, 126, 650, 1200, 1057, 296, 967, 316, 1416, 1292, 202, 753, 633, 908, 973, 780, 1212, 371, 1097, 824, 530, 775, 1173, 1547, 90, 222, 1036, 432, 282, 1344, 391, 449, 510, 1218, 438, 399, 600, 516, 1153, 140, 455, 195, 618, 730, 1557, 1190, 366, 673, 188, 1147, 414, 738, 724, 1465, 1455, 1337, 114, 577, 201, 227, 1495, 1318, 745, 826, 601, 1214, 598, 1584, 285, 960, 509, 788, 672, 488, 664, 1439, 521, 74, 747, 757, 173, 933, 492, 1492, 1254, 1473, 731, 198, 1405, 0, 1076, 125, 207, 623, 111, 991, 1387, 1131, 570, 1442, 1612, 208, 312, 1168, 40, 462, 1604, 971, 1610, 556, 1078, 1070, 1016, 835, 695, 529, 452, 1460, 437, 658, 1633, 1289, 739, 758, 279, 611, 1048, 981, 783, 496, 1299, 785, 1109, 787, 369, 13, 1231, 725, 143, 827, 716, 1229, 1045, 805, 1392, 353, 1365, 1230, 949, 1136, 305, 1517, 394, 956, 236, 610, 1540, 107, 447, 1094, 1111, 523, 1419, 1008, 918, 703, 655, 954, 946, 1570, 109, 289, 154, 472, 1463, 1579, 261, 728, 998, 866, 968, 31, 368, 709, 1352, 495, 1259, 1390, 405, 883, 583, 573, 1559, 874, 370, 671, 1296, 1106, 93, 701, 1062, 1331, 1532, 1142, 132, 419, 1059, 531, 605, 1140, 1424, 668, 1197, 630, 446, 943, 1067, 381, 1128, 505, 592, 1246, 477, 986, 812, 11, 887, 1422, 726, 1375, 871, 1635, 1090, 147, 1431, 181, 1415, 1606, 914, 1340, 1104, 1192, 513, 690, 262, 1477, 243, 1232, 560, 680, 71, 572, 342, 1043, 49, 484, 905, 130, 1189, 670, 561, 448, 1346, 1158, 1593, 101, 58, 1243, 221, 974, 480, 1301, 401, 882, 428, 876, 1150, 720, 47, 297, 1052, 1087, 1233, 475, 476, 1339, 797, 839, 1304, 310, 1265, 844, 30, 168, 1379, 254, 1035, 172, 904, 1186, 9, 322, 1178, 1202, 1626, 1341, 1406, 1242, 567, 1012, 1634, 624, 781, 875, 358, 1622, 1408, 8, 1262, 1614, 700, 877, 924, 1576, 778, 149, 931, 59, 15, 1400, 669, 774, 278, 1330, 773, 849, 293, 1410, 1409, 1371, 141, 1536, 842, 1594, 352, 619, 424, 494, 697, 42, 440, 1420, 1332, 56, 597, 1294, 862, 1461, 520, 997, 552, 1527, 1376, 490, 1183, 749, 1187, 269, 1510, 657, 1228, 1637, 453, 637, 868, 595, 48, 1462, 482, 989, 1288, 1480, 999, 1050, 1287, 1457, 1093, 119, 1155, 406, 682, 729, 491, 1429, 1512, 732, 193, 1001, 1157, 1026, 1578, 273, 384, 1476, 1126, 1055, 1556, 263, 460, 32, 1064, 674, 1572, 1149, 398, 791, 662, 1545, 1573, 1464, 1226, 1300, 436, 1015, 564, 962, 1485, 541, 1082, 37, 1430, 1474, 502, 576, 412, 982, 759, 860, 1080, 134, 409, 1079, 1077, 1328, 1445, 1203, 250, 800, 524, 1577, 1191, 483, 945, 1185, 1193, 762, 1544, 1113, 1270, 28, 894, 562, 66, 347, 1313, 1333, 784, 677, 854, 1303, 105, 522, 708, 1284, 454, 395, 1603, 1504, 548, 1290, 1574, 648, 514, 1484, 84, 990, 215, 626, 517, 199, 1459, 1386, 1471, 833, 413, 1336, 533, 1568, 288, 212, 750, 1499, 776, 1599, 804, 1205, 816, 1271, 1433, 390, 1058, 540, 3, 921, 1069, 983, 727, 947, 1596, 1309, 1291, 895, 1443, 1143, 127, 211, 156, 1216, 1264, 1558, 1498, 470, 2, 280, 443, 1224, 1338, 1250, 733, 1176, 34, 640, 580, 1211, 1600, 1549, 67, 27, 1194, 1505, 646, 988, 569, 616, 1382, 1595, 14, 1285, 1280, 425, 802, 1003, 333, 52, 388, 175, 900, 1282, 940, 22, 612, 350, 1554, 817, 1397, 35, 698, 608, 1210, 116, 1438, 1177, 659, 589, 1279, 474, 292, 1063, 382, 1617, 303, 167, 793, 864, 232, 131, 260, 1244, 1325, 112, 536, 993, 4, 392, 741, 1587, 638, 1619, 373, 157, 165, 767, 1160, 1380, 39, 1267, 1592, 1020, 935, 756, 389, 1273, 621, 1281, 719, 284, 1180, 1277, 364, 1317, 1251, 256, 410, 166, 497, 83, 1537, 1586, 247, 649, 218, 500, 1215, 1276, 1066, 734, 164, 335, 489, 158, 846]\n",
      "###################cap done, start swapping##############\n",
      "Swapped sent ids: [172, 263, 751, 568, 1311, 247, 124, 402, 236, 1076, 70, 270, 1466, 154, 1381, 560, 1628, 15, 510, 929, 524, 81, 832, 66, 769, 1244, 86, 680, 330, 1495, 1114, 1025, 1386, 1498, 548, 1052, 551, 180, 594, 1402, 592, 1126, 1320, 956, 1267, 1250, 113, 83, 1217, 471, 760, 641, 614, 497, 1600, 1468, 1535, 954, 834, 410, 800, 251, 657, 1606, 907, 620, 215, 377, 909, 1297, 501, 868, 100, 1209, 1131, 339, 1191, 1264, 376, 73, 123, 787, 1069, 1308, 11, 921, 225, 1360, 1520, 724, 1479, 21, 1449, 1339, 646, 554, 582, 1368, 1359, 375, 1353, 920, 1196, 1119, 1395, 155, 824, 99, 1634, 26, 1483, 443, 1344, 1555, 472, 946, 852, 1594, 193, 305, 1365, 198, 590, 318, 782, 281, 337, 1103, 1101, 436, 1546, 883, 104, 764, 186, 518, 283, 555, 634, 1512, 1203, 1143, 418, 1272, 1009, 252, 890, 48, 1307, 889, 1285, 980, 1121, 1452, 50, 652, 1633, 474, 1327, 1509, 1469, 1074, 1130, 610, 304, 747, 1323, 1275, 328, 878, 51, 1618, 188, 755, 496, 638, 1283, 855, 309, 65, 126, 393, 248, 1190, 1169, 1542, 1002, 342, 797, 1260, 219, 1593, 1201, 608, 626, 195, 1598, 461, 210, 345, 716, 1585, 1461, 441, 291, 727, 1087, 360, 1316, 1392, 315, 483, 1624, 1418, 916, 13, 1328, 1000, 1023, 208, 1166, 940, 1490, 1019, 295, 1288, 1574, 1187, 1549, 269, 475, 1465, 19, 966, 421, 71, 1006, 768, 319, 1177, 1374, 140, 1281, 1502, 1366, 976, 1214, 302, 445, 314, 227, 994, 1292, 1220, 784, 547, 1287, 1470, 1051, 706, 150, 334, 468, 1200, 121, 183, 1247, 720, 286, 300, 243, 322, 1330, 448, 1134, 600, 1012, 1252, 1586, 1210, 199, 1064, 149, 456, 1551, 256, 791, 1072, 804, 848, 991, 201, 240, 731, 235, 209, 1627, 642, 502, 1496, 1110, 623, 1447, 8, 1113, 1127, 532, 1430, 1145, 604, 364, 265, 630, 1568, 1635, 1393, 1261, 1137, 621, 258, 176, 310, 609, 538, 1427, 1453, 470, 1342, 432, 435, 895, 223, 1136, 739, 119, 1230, 1062, 1111, 1157, 1326, 465, 336, 1522, 509, 1589, 389, 1599, 463, 564, 654, 1189, 1301, 353, 1504, 417, 1314, 365, 116, 391, 1176, 351, 1263, 440, 1298, 1501, 1484, 686, 485, 1597, 544, 1384, 386, 1497, 43, 78, 469, 1390, 1003, 128, 520, 917, 1611, 192, 1371, 1576, 558, 1625, 1411, 1579, 395, 1325, 487, 902, 413, 1223, 819, 1125, 833, 466, 893, 942, 278, 1174, 202, 292, 1480, 1343, 567, 338, 383, 885, 174, 439, 603, 246, 778, 949, 1249, 1031, 272, 1040, 1591, 1147, 1265, 1561, 1066, 47, 61, 617, 757, 596, 1167, 1417, 656, 129, 1293, 284, 1182, 1248, 1552, 1467, 939, 170, 1356, 1079, 1608, 147, 118, 166, 72, 1367, 399, 332, 1175, 676, 704, 645, 321, 986, 2, 160, 1055, 1218, 733, 294, 1122, 1396, 350, 1548, 1409, 1071, 288, 659, 1584, 1319, 1089, 238, 1093, 289, 1238, 601, 20, 845, 990, 114, 1269, 426, 1273, 1424, 438, 828, 374, 250, 271, 218, 1482, 1614, 1456, 1619, 1422, 1063, 752, 1115, 1499, 297, 1193, 1437, 1058, 490, 793, 1309, 971, 586, 570, 28, 397, 745, 74, 1020, 1602, 1095, 1073, 1027, 1313, 785, 854, 1378, 35, 1578, 1630, 25, 1239, 273, 370, 1569, 1440, 915, 1472, 372, 758, 1350, 191, 875, 1199, 987, 925, 948, 110, 211, 910, 1332, 557, 563, 93, 196, 313, 1207, 1322, 619, 24, 306, 1331, 816, 1601, 763, 618, 428, 1081, 444, 1413, 1629, 799, 1401, 613, 1616, 324, 593, 167, 624, 1268, 753, 108, 598, 407, 756, 95, 527, 1476, 633, 1271, 897, 130, 1227, 1011, 299, 625, 658, 674, 1154, 276, 1318, 1153, 1537, 899, 1202, 1056, 274, 1290, 896, 37, 1039, 1240, 923, 965, 1433, 1412, 1133, 1364, 1222, 543, 513, 1274, 628, 1194, 957, 109, 1208, 1638, 1583, 871, 59, 814, 699, 112, 282, 774, 974, 1603, 622, 212, 431, 820, 627, 1204, 1348, 542, 850, 481, 259, 244, 523, 92, 830, 857, 1489]\n",
      "\n",
      "#########swapping done, start insertion##################\n",
      "[1492, 1565, 544, 359, 230, 1006, 1212, 151, 783, 1438, 956, 1592, 781, 1220, 763, 1161, 912, 517, 1064, 155, 128, 1553, 1545, 620, 263, 662, 1573, 586, 18, 231, 1214, 1315, 1306, 270, 1347, 16, 1076, 755, 1256, 1613, 516, 183, 967, 302, 216, 1268, 234, 1471, 1258, 983, 1555, 163, 707, 1163, 1465, 636, 645, 1429, 626, 108, 821, 0, 179, 140, 170, 604, 521, 729, 1277, 545, 1313, 59, 82, 1608, 1040, 1511, 1551, 1554, 1240, 158, 375, 1181, 149, 21, 206, 632, 1196, 416, 60, 605, 31, 978, 1467, 20, 728, 1372, 507, 327, 1483, 145, 512, 135, 459, 1130, 374, 1445, 153, 1615, 514, 1391, 1532, 530, 93, 1058, 1498, 1319, 486, 141, 1595, 164, 1125, 1323, 584, 450, 1132, 1171, 928, 1245, 735, 1153, 1531, 373, 3, 1448, 274, 241, 657, 523, 865, 1032, 1600, 699, 824, 1050, 299, 1389, 854, 256, 1469, 1441, 906, 1321, 969, 513, 617, 1416, 287, 1496, 1409, 465, 452, 191, 556, 886, 1451, 1506, 476, 537, 935, 534, 861, 930, 676, 828, 609, 404, 48, 659, 870, 219, 799, 1269, 1048, 1336, 1628, 813, 1152, 1611, 1257, 1159, 972, 1141, 994, 387, 1371, 923, 300, 734, 621, 1392, 260, 817, 445, 1106, 1606, 1626, 1480, 1183, 1403, 739, 1293, 127, 1569, 853, 1255, 291, 630, 1010, 698, 1580, 926, 1116, 314, 1331, 55, 765, 1273, 1274, 660, 213, 358, 672, 1028, 1128, 1103, 857, 386, 197, 1098, 288, 90, 470, 110, 504, 1312, 866, 455, 1543, 655, 461, 654, 841, 1614, 1394, 590, 1461, 188, 1055, 1204, 1473, 890, 204, 538, 1456, 84, 1020, 674, 759, 1631, 1185, 666, 1100, 653, 596, 192, 893, 481, 1129, 116, 1039, 175, 671, 7, 750, 758, 638, 1113, 33, 125, 64, 1252, 904, 406, 139, 473, 639, 1405, 1135, 195, 1060, 199, 744, 1073, 743, 533, 341, 535, 1235, 917, 94, 722, 1140, 50, 1148, 624, 1180, 1259, 737, 1292, 71, 79, 301, 1167, 1061, 1577, 376, 78, 107, 130, 877, 546, 13, 568, 482, 391, 1262, 1105, 941, 749, 1410, 839, 1178, 402, 1063, 1101, 1218, 324, 1236, 753, 472, 791, 1494, 113, 1276, 1120, 1502, 1560, 75, 909, 167, 1586, 811, 943, 355, 1300, 369, 133, 500, 221, 1049, 541, 440, 981, 181, 932, 1094, 1379, 1387, 887, 392, 1226, 1182, 1354, 1031, 1617, 680, 622, 329, 143, 318, 1434, 612, 460, 1364, 1356, 748, 111, 126, 431, 704, 401, 357, 19, 899, 131, 794, 132, 1475, 365, 973, 600, 1575, 1224, 990, 1117, 1572, 58, 211, 339, 348, 364, 333, 1491, 74, 1115, 72, 569, 112, 436, 311, 1282, 942, 6, 492, 326, 1005, 1515, 419, 690, 44, 1420, 61, 237, 1046, 1378, 1197, 1016, 1271, 747, 754, 96, 1495, 1102, 1127, 1478, 815, 464, 845, 161, 1512, 987, 1489, 1285, 1078, 851, 1609, 1582, 1520, 1623, 152, 825, 678, 810, 443, 1108, 686, 1485, 399, 1525, 99, 1484, 205, 277, 1069, 53, 1192, 1056, 122, 816, 611, 1230, 668, 607, 1270, 487, 396, 643, 177, 1601, 619, 1232, 1149, 1504, 508, 1222, 14, 1497, 1510, 1200, 1059, 91, 310, 1346, 100, 174, 77, 936, 303, 273, 1209, 706, 240, 1596, 1605, 1035, 1213, 1390, 1260, 593, 786, 789, 22, 1534, 57, 776, 1397, 1583, 1566, 852, 309, 393, 576, 1453, 1603, 876, 319, 1358, 446, 420, 1472, 331, 1225, 1210, 1635, 1440, 467, 323, 1242, 378, 203, 896, 10, 730, 1550, 1593, 214, 1089, 1486, 1075, 589, 849, 360, 1316, 616, 898, 1558, 1144, 1637, 1584, 1029, 123, 633, 1154, 581, 1401, 892, 1431, 1238, 1330, 1205, 1536, 985, 717, 438, 1359, 1150, 17, 1377, 2, 585, 222, 4, 1517, 858, 770, 283, 922, 1041, 1019, 712, 863, 921, 361, 1124, 986, 346, 583, 1012, 1423, 345, 915, 826, 1340, 891, 70, 588, 1329, 663, 806, 1071, 974, 529, 1581, 1464, 176, 306, 578, 920, 1172, 1590, 1082, 477, 1527, 1184, 1290, 121, 714, 289, 1571, 1638, 24, 779, 1201, 1079, 463]\n",
      "#########insertion done, start deletion##################\n",
      "\n",
      "perturbations done, start merging lists\n",
      "final df\n",
      "Capped sent ids: [24, 386, 345, 681, 520, 640, 493, 239, 588, 653, 673, 42, 384, 625, 263, 642, 413, 158, 361, 556, 28, 383, 485, 146, 89, 662, 320, 410, 102, 344, 174, 504, 389, 101, 128, 43, 270, 592, 692, 131, 252, 472, 111, 110, 108, 180, 536, 466, 379, 6, 85, 617, 11, 295, 399, 659, 488, 202, 341, 654, 428, 471, 589, 553, 268, 371, 190, 58, 267, 578, 284, 340, 116, 501, 476, 571, 601, 593, 370, 308, 406, 7, 76, 157, 445, 369, 411, 47, 486, 254, 585, 67, 310, 650, 199, 671, 32, 405, 120, 299, 39, 201, 442, 200, 62, 126, 304, 325, 338, 705, 162, 60, 590, 569, 253, 9, 619, 115, 143, 0, 342, 457, 668, 523, 434, 125, 356, 68, 161, 134, 510, 59, 380, 645, 496, 535, 634, 587, 154, 80, 498, 683, 324, 701, 212, 447, 259, 404, 38, 621, 86, 148, 376, 103, 543, 185, 100, 388, 463, 147, 339, 426, 122, 186, 497, 231, 323, 245, 397, 687, 533, 78, 84, 574, 121, 461, 97, 286, 302, 1, 19, 552, 670, 207, 541, 94, 522, 636, 643, 170, 293, 402, 483, 666, 205, 311, 278, 655, 183, 16, 609, 61, 12, 57, 291, 316, 694, 261, 249, 114, 46, 188, 83, 573, 81, 660, 191, 366, 691, 372, 387, 505, 473, 391, 56, 576, 331, 173, 633, 234, 547, 73, 74, 560, 437, 581, 456, 598, 365, 189, 689, 285, 297, 25, 179, 594, 30, 506, 508, 385, 187, 149, 669, 4, 579, 243, 495, 414, 626, 661, 360, 423, 649, 235, 133, 271, 475, 644, 37, 91, 196, 150, 240, 514, 604, 274, 352, 393, 429, 567, 605, 332, 362, 440]\n",
      "###################cap done, start swapping##############\n",
      "Swapped sent ids: [163, 61, 22, 675, 591, 555, 195, 638, 238, 28, 582, 483, 188, 525, 438, 0, 707, 170, 347, 269, 626, 357, 164, 104, 189, 524, 609, 709, 30, 431, 552, 595, 338, 263, 172, 489, 500, 71, 138, 618, 320, 36, 210, 503, 247, 700, 119, 459, 468, 302, 160, 305, 453, 7, 20, 577, 401, 81, 141, 233, 493, 570, 158, 90, 301, 85, 236, 16, 69, 37, 122, 490, 294, 151, 11, 331, 328, 352, 488, 235, 278, 58, 562, 10, 572, 40, 587, 466, 574, 339, 680, 98, 571, 499, 664, 155, 139, 156, 264, 80, 629, 113, 410, 364, 547, 146, 393, 523, 76, 477, 17, 27, 68, 694, 380, 99, 448, 688, 652, 325, 508, 140, 593, 556, 4, 228, 26, 537, 43, 130, 53, 291, 290, 360, 418, 348, 371, 129, 428, 449, 397, 383, 491, 97, 399, 96, 33, 693, 351, 116, 409, 270, 350, 706, 687, 5, 390, 478, 12, 54, 72, 87, 519, 487, 565, 417, 420, 433, 465, 633, 539, 534, 162, 94, 322, 254, 670, 620, 495, 513, 23, 88, 592, 476, 429, 310, 312, 369, 121, 110, 241, 147, 309, 521, 636, 330, 526, 370, 45, 612, 567, 423, 588, 183, 589, 686, 307, 651, 321, 253, 159, 426, 533, 480, 501, 199, 522, 300, 215, 484, 619, 38, 363, 462, 176, 268, 554, 665, 627, 144, 52, 255, 392, 204, 288, 647, 705, 118, 354, 391, 701, 568, 656, 650, 208, 378, 249, 125, 545, 517, 234, 566, 70, 166, 659, 381, 368, 553, 224, 442, 373, 101, 679, 599, 316, 520, 646, 103, 356, 187, 181, 464, 576, 342, 89, 440, 496, 394, 642, 641, 272, 207, 13, 671]\n",
      "\n",
      "#########swapping done, start insertion##################\n",
      "[468, 636, 433, 104, 278, 78, 466, 11, 95, 17, 635, 702, 364, 476, 54, 500, 620, 264, 441, 631, 533, 407, 316, 528, 109, 208, 367, 304, 383, 174, 345, 602, 250, 337, 205, 661, 279, 202, 567, 98, 226, 465, 84, 227, 71, 597, 211, 4, 380, 305, 467, 526, 350, 506, 159, 135, 7, 346, 120, 488, 20, 549, 361, 558, 369, 570, 478, 291, 498, 214, 641, 569, 12, 681, 491, 656, 654, 322, 529, 585, 389, 197, 184, 458, 175, 648, 287, 18, 689, 655, 514, 319, 662, 456, 101, 288, 603, 512, 293, 591, 68, 231, 706, 363, 418, 308, 129, 96, 80, 196, 215, 47, 247, 19, 244, 29, 708, 704, 646, 37, 413, 647, 609, 41, 266, 481, 682, 519, 325, 471, 302, 608, 452, 595, 408, 382, 340, 557, 50, 221, 688, 82, 699, 457, 301, 181, 539, 273, 191, 499, 673, 262, 439, 299, 290, 153, 164, 696, 709, 13, 690, 232, 482, 182, 659, 511, 544, 701, 409, 33, 492, 588, 48, 376, 67, 560, 72, 86, 32, 284, 28, 611, 190, 160, 565, 230, 87, 630, 326, 246, 53, 695, 429, 146, 497, 450, 448, 114, 249, 289, 687, 36, 573, 627, 613, 253, 347, 460, 392, 622, 480, 65, 255, 46, 537, 122, 477, 222, 154, 147, 105, 137, 657, 183, 22, 90, 371, 535, 43, 145, 177, 171, 116, 561, 703, 427, 236, 223, 52, 79, 64, 555, 126, 381, 679, 339, 496, 81, 281, 552, 691, 551, 697, 136, 435, 619, 283, 583, 542, 248, 639, 400, 335, 483, 282, 254, 169, 60, 6, 563, 229, 547, 216, 357, 300, 141, 348, 272, 463, 474, 652, 568, 178, 395]\n",
      "#########insertion done, start deletion##################\n",
      "\n",
      "perturbations done, start merging lists\n",
      "final df\n",
      "Capped sent ids: [436, 803, 7, 230, 1100, 409, 667, 703, 340, 71, 169, 226, 223, 267, 492, 572, 940, 658, 42, 557, 242, 565, 5, 322, 89, 437, 206, 157, 736, 69, 1187, 1084, 41, 1104, 129, 1135, 686, 1088, 347, 1065, 1157, 140, 389, 908, 469, 426, 538, 836, 853, 84, 235, 233, 143, 1185, 183, 623, 165, 330, 648, 130, 784, 459, 603, 450, 637, 906, 700, 681, 338, 1130, 1083, 225, 833, 1002, 956, 237, 299, 924, 697, 1170, 1093, 512, 520, 616, 891, 828, 611, 1103, 151, 1145, 496, 1106, 66, 526, 255, 239, 959, 727, 21, 1160, 712, 578, 786, 72, 462, 562, 1164, 414, 1043, 260, 363, 135, 824, 1119, 1177, 624, 698, 234, 890, 181, 1097, 709, 728, 718, 75, 592, 407, 609, 741, 855, 790, 228, 258, 1038, 740, 1037, 354, 352, 666, 1118, 144, 606, 1070, 50, 1072, 984, 262, 202, 418, 605, 68, 779, 395, 378, 654, 530, 927, 1181, 1138, 247, 383, 1168, 549, 851, 662, 56, 115, 1052, 953, 37, 1071, 717, 634, 8, 123, 375, 268, 466, 107, 796, 208, 33, 850, 245, 1090, 500, 201, 639, 422, 581, 1079, 593, 133, 345, 826, 1176, 1080, 57, 367, 1086, 965, 751, 106, 900, 365, 250, 870, 508, 692, 410, 511, 515, 1027, 220, 815, 197, 440, 1032, 757, 271, 763, 766, 837, 904, 502, 210, 454, 738, 58, 688, 67, 705, 638, 222, 1019, 124, 86, 620, 859, 695, 1159, 689, 548, 503, 209, 4, 792, 195, 789, 678, 1057, 802, 396, 878, 433, 806, 569, 665, 746, 26, 513, 498, 207, 490, 903, 643, 472, 312, 1151, 937, 263, 816, 1099, 1096, 684, 463, 1141, 417, 166, 269, 308, 642, 467, 860, 120, 196, 109, 679, 232, 305, 929, 494, 185, 164, 248, 791, 1016, 1006, 1040, 640, 424, 774, 382, 240, 60, 292, 449, 888, 74, 544, 600, 19, 114, 957, 694, 983, 504, 716, 617, 651, 221, 1056, 1112, 894, 1050, 1034, 27, 274, 880, 743, 374, 316, 217, 872, 85, 702, 199, 342, 368, 537, 895, 1045, 559, 925, 148, 708, 505, 997, 864, 1127, 582, 902, 13, 102, 550, 982, 948, 589, 188, 1062, 257, 219, 730, 190, 264, 516, 287, 958, 97, 128, 996, 91, 936, 1015, 787, 153, 1124, 812, 371, 81, 319, 289, 421, 215, 464, 799, 149, 558, 721, 734, 286, 928, 444, 92, 969, 103, 1152, 561, 949, 380, 476, 685, 672, 427, 6, 933, 992, 162, 961, 0, 1188, 1115, 674, 392, 261, 435, 45, 972, 442, 113, 307, 167, 590, 525, 1069, 772, 576, 187, 229, 403, 243, 938, 635, 121, 224, 474, 156, 845, 823, 313, 343, 193, 661, 755, 612, 1165, 586, 408, 839, 77, 735, 1126, 28, 1020, 23, 1180, 1114, 866, 691, 377, 879, 646, 477, 883, 461, 521, 82, 428, 722, 161, 471, 528, 1174, 1041, 567, 429, 834, 518, 373, 975]\n",
      "###################cap done, start swapping##############\n",
      "Swapped sent ids: [117, 818, 631, 920, 1131, 1169, 94, 748, 53, 911, 1146, 513, 1056, 907, 476, 295, 985, 664, 899, 322, 364, 229, 641, 812, 572, 609, 444, 442, 349, 774, 742, 1097, 1156, 588, 672, 576, 719, 132, 406, 759, 628, 523, 269, 1145, 966, 13, 310, 1164, 715, 806, 315, 4, 687, 697, 529, 703, 616, 637, 412, 593, 794, 374, 799, 614, 738, 1098, 200, 1134, 443, 387, 312, 372, 1177, 950, 866, 467, 389, 50, 108, 473, 858, 825, 562, 339, 819, 1101, 69, 390, 331, 168, 1014, 949, 632, 1186, 560, 1082, 317, 578, 859, 1118, 872, 96, 386, 1125, 896, 784, 231, 602, 998, 68, 721, 33, 1170, 264, 691, 1142, 789, 162, 561, 1001, 694, 1109, 757, 690, 937, 683, 1078, 139, 51, 431, 305, 253, 135, 3, 81, 31, 551, 381, 743, 1116, 1017, 318, 104, 10, 30, 724, 427, 277, 138, 320, 1087, 215, 621, 1049, 692, 662, 327, 846, 1136, 638, 462, 814, 425, 862, 848, 655, 958, 1010, 956, 923, 348, 880, 319, 548, 729, 679, 263, 392, 976, 1008, 568, 779, 854, 1029, 993, 183, 24, 452, 125, 167, 516, 363, 216, 1163, 625, 864, 5, 904, 926, 995, 330, 887, 370, 711, 1120, 903, 145, 797, 268, 915, 684, 1171, 262, 150, 941, 55, 136, 92, 515, 902, 280, 844, 520, 20, 188, 171, 76, 509, 1085, 777, 78, 728, 1007, 102, 740, 86, 928, 1178, 1111, 222, 830, 1067, 42, 95, 67, 544, 566, 983, 27, 204, 939, 599, 1115, 869, 407, 1066, 1052, 1037, 701, 1055, 173, 174, 851, 111, 963, 221, 250, 266, 286, 230, 1064, 401, 943, 498, 409, 1110, 149, 750, 18, 1121, 306, 313, 1068, 530, 1000, 651, 653, 1083, 48, 645, 541, 220, 675, 1095, 922, 56, 464, 626, 1024, 491, 802, 147, 557, 973, 564, 1023, 213, 879, 749, 241, 597, 346, 160, 170, 11, 97, 292, 1132, 361, 186, 852, 378, 19, 1003, 1016, 988, 532, 260, 468, 930, 84, 446, 342, 365, 547, 1191, 90, 761, 778, 929, 510, 706, 1063, 203, 569, 661, 785, 205, 643, 734, 271, 992, 75, 546, 505, 630, 695, 214, 0, 758, 640, 1133, 288, 699, 861, 226, 291, 25, 1160, 298, 22, 161, 1009, 45, 856, 575, 1060, 192, 232, 889, 1094, 957, 883, 972, 141, 708, 1012, 776, 969, 1092, 12, 813, 66, 577, 615, 1020, 1069, 382, 481, 762, 1147, 209, 1025, 800, 668, 659, 493, 245, 1086, 486, 172, 535, 388, 437, 574, 1104, 1198, 408, 1089, 164, 417, 772, 1015, 919, 991, 925, 905, 648, 725, 1137, 333, 507, 1199, 543, 371, 603, 940, 512, 895, 552, 355, 540, 1054, 404, 441, 827, 169, 157, 537, 1021, 448, 613, 228, 994, 681, 835, 521, 124, 775, 433, 961, 323, 474, 571, 439, 635, 982, 1149, 335, 459, 912, 73, 1077, 1033, 1057, 166, 1035, 287, 275, 503]\n",
      "\n",
      "#########swapping done, start insertion##################\n",
      "[281, 352, 1074, 243, 652, 339, 560, 758, 828, 39, 715, 731, 870, 787, 663, 797, 426, 1053, 273, 213, 44, 459, 697, 307, 576, 706, 1069, 981, 917, 1125, 1188, 262, 549, 84, 61, 635, 480, 1197, 617, 521, 74, 412, 235, 452, 859, 675, 902, 668, 97, 398, 73, 537, 732, 926, 1128, 161, 344, 400, 1097, 900, 871, 354, 256, 47, 826, 189, 710, 768, 301, 982, 872, 360, 120, 319, 1109, 304, 807, 60, 1027, 920, 1169, 149, 381, 300, 378, 64, 946, 124, 516, 559, 825, 1009, 26, 183, 910, 569, 55, 1031, 1130, 767, 875, 86, 1149, 504, 1133, 40, 579, 993, 966, 1018, 464, 473, 359, 386, 1191, 857, 1006, 1105, 1193, 326, 92, 659, 291, 999, 615, 676, 771, 712, 144, 952, 414, 565, 865, 582, 1054, 242, 1060, 278, 103, 897, 602, 527, 525, 447, 640, 172, 1171, 313, 591, 1058, 930, 783, 883, 389, 13, 777, 502, 728, 1084, 469, 267, 1052, 1087, 750, 703, 245, 566, 369, 305, 1042, 738, 423, 842, 1021, 558, 260, 691, 749, 56, 832, 632, 1168, 964, 810, 950, 482, 20, 434, 817, 218, 690, 157, 196, 23, 215, 96, 402, 976, 1151, 727, 335, 765, 572, 629, 1039, 223, 350, 992, 507, 70, 27, 512, 440, 219, 1158, 1200, 647, 171, 170, 353, 1117, 1198, 318, 1050, 737, 785, 801, 116, 388, 1091, 666, 195, 837, 764, 110, 1025, 1107, 692, 913, 845, 1199, 881, 143, 4, 788, 574, 33, 841, 338, 953, 289, 535, 714, 497, 320, 36, 831, 1061, 370, 108, 619, 101, 1102, 162, 634, 494, 1132, 1045, 212, 931, 587, 164, 852, 761, 1131, 1155, 446, 1111, 745, 581, 850, 410, 564, 520, 707, 478, 623, 155, 62, 323, 233, 444, 1161, 29, 677, 660, 1142, 199, 519, 17, 890, 1062, 358, 22, 391, 51, 50, 387, 536, 918, 403, 500, 200, 21, 723, 936, 971, 879, 1116, 760, 322, 815, 889, 528, 93, 325, 556, 695, 903, 425, 495, 1083, 543, 188, 479, 1095, 741, 38, 671, 205, 441, 589, 963, 864, 1044, 965, 772, 1163, 139, 192, 1148, 957, 607, 12, 470, 696, 1153, 799, 272, 542, 254, 5, 328, 980, 684, 1092, 1038, 904, 1160, 484, 510, 643, 280, 680, 134, 805, 726, 475, 367, 138, 363, 538, 78, 1136, 840, 974, 465, 665, 990, 49, 28, 970, 11, 1073, 618, 356, 137, 798, 283, 24, 111, 782, 95, 98, 453, 811, 404, 603, 735, 306, 544, 580, 827, 191, 1104, 969, 858, 616, 1195, 984, 341, 99, 167, 713, 1182, 595, 294, 1037, 1184, 1162, 265, 1071, 1165, 1002, 662, 1055, 806, 593, 31, 229, 79, 1077, 773, 708, 689, 1040, 336, 724, 770, 1001, 177, 450, 240, 34, 555, 68, 127, 258, 979, 1012, 1156, 261, 594, 488, 1146, 330, 1065, 75, 1019, 1172, 717, 1075, 340, 830, 351, 292, 19, 421, 435, 947]\n",
      "#########insertion done, start deletion##################\n",
      "\n",
      "perturbations done, start merging lists\n",
      "final df\n",
      "File writing complete!\n"
     ]
    }
   ],
   "source": [
    "write_perturbed_files(train_data, dev_data, test_data, P_SENT, P_WORDS, N_LETTERS, PATH_PERTURBED_TRAIN, \n",
    "                      PATH_PERTURBED_DEV, PATH_PERTURBED_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True # dw about it\n",
    "def tokenize_and_align_labels(dataset, word_column, tag_column, tokenizer):\n",
    "    '''\n",
    "    Function tokenizes sentences and aligns the subword tokens with the labels\n",
    "    '''\n",
    "    tokenized_inputs = tokenizer(dataset[word_column].tolist(), truncation=True, is_split_into_words=True, padding = True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset[tag_column]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_Len = np.min([len(list_) for list_ in train_data[\"words\"]])\n",
    "min_Len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data and align the labels. We do this because of the subwords tokenization to get a label per token instead of per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_and_align_labels(train_data, \"words\", \"tag_idx\", tokenizer)\n",
    "tokenized_dev_data = tokenize_and_align_labels(dev_data, \"words\", \"tag_idx\", tokenizer)\n",
    "tokenized_test_data = tokenize_and_align_labels(test_data, \"words\", \"tag_idx\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
