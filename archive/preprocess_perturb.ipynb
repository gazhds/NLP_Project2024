{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_file(path):\n",
    "    \n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[2:]\n",
    "        \n",
    "    for line in lines:\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if line: # if line is not an empty line\n",
    "            tok = line.split('\\t')\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[3])\n",
    "            \n",
    "        else:\n",
    "            if current_words:\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            \n",
    "            \n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['words', 'tags'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'words', 'tags']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEO’s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              words  \\\n",
       "0   0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1   1  [#Volunteers, are, key, members, of, #CHEO’s, ...   \n",
       "2   2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3   3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4   4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "\n",
       "                                                tags  \n",
       "0  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3                        [O, O, O, O, O, O, O, O, O]  \n",
       "4   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = read_bio_file(\"train.bio\")\n",
    "dev_data = read_bio_file(\"dev.bio\")\n",
    "test_data = read_bio_file(\"test.bio\")\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting tag => index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-ORG': 1, 'B-PER': 2, 'B-LOC': 3, 'I-PER': 4, 'B-MISC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word is None or word == self.pad_unk:\n",
    "            return None\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                idx = len(self.idx2word)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word.append(word)\n",
    "                return idx\n",
    "            else:\n",
    "                return None\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "label_indices = Vocab()\n",
    "tags_column = train_data[\"tags\"]\n",
    "\n",
    "for tags in tags_column:\n",
    "    for tag in tags:\n",
    "        label_indices.getIdx(tag, add=True)\n",
    "\n",
    "print(label_indices.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we map the tags to indices and add them as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEO’s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              words  \\\n",
       "0   0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1   1  [#Volunteers, are, key, members, of, #CHEO’s, ...   \n",
       "2   2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3   3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4   4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                        [O, O, O, O, O, O, O, O, O]   \n",
       "4   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                             tag_idx  \n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tag_idx'] = train_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "dev_data['tag_idx'] = dev_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "test_data['tag_idx'] = test_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, padding=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbations\n",
    "Functions for perturbation of the dataset. We make them before tokenization for a reason. Should not change the length of the sentence (tags should still correspond)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEO’s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>1634</td>\n",
       "      <td>[RT, @USER1701, :, FT, ISLAND, -, I, Hope, (, ...</td>\n",
       "      <td>[O, O, O, O, B-PER, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[0, 0, 0, 0, 2, 0, 5, 7, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>1635</td>\n",
       "      <td>[@USER1321, @USER2526, Probably, ., He, is, n'...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>1636</td>\n",
       "      <td>[RT, @USER1920, :, @USER1260, @USER2624, it, '...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>1637</td>\n",
       "      <td>[You, have, that, right, ,, nor, do, they, int...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>1638</td>\n",
       "      <td>[RT, @USER364, :, Donald, Trump, is, the, Clea...</td>\n",
       "      <td>[O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, B...</td>\n",
       "      <td>[0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1639 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              words  \\\n",
       "0        0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1        1  [#Volunteers, are, key, members, of, #CHEO’s, ...   \n",
       "2        2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3        3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4        4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "...    ...                                                ...   \n",
       "1634  1634  [RT, @USER1701, :, FT, ISLAND, -, I, Hope, (, ...   \n",
       "1635  1635  [@USER1321, @USER2526, Probably, ., He, is, n'...   \n",
       "1636  1636  [RT, @USER1920, :, @USER1260, @USER2624, it, '...   \n",
       "1637  1637  [You, have, that, right, ,, nor, do, they, int...   \n",
       "1638  1638  [RT, @USER364, :, Donald, Trump, is, the, Clea...   \n",
       "\n",
       "                                                   tags  \\\n",
       "0     [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...   \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2                     [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [O, O, O, O, O, O, O, O, O]   \n",
       "4      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "...                                                 ...   \n",
       "1634  [O, O, O, O, B-PER, O, B-MISC, I-MISC, O, O, O...   \n",
       "1635               [O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1636                        [O, O, O, O, O, O, O, O, O]   \n",
       "1637  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1638  [O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, B...   \n",
       "\n",
       "                                                tag_idx  \n",
       "0     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3                           [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                                 ...  \n",
       "1634               [0, 0, 0, 0, 2, 0, 5, 7, 0, 0, 0, 0]  \n",
       "1635               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1636                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1637  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1638  [0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, ...  \n",
       "\n",
       "[1639 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add your functions here\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chunky',\n",
       " 'breads',\n",
       " 'from',\n",
       " '@USER109',\n",
       " 'are',\n",
       " 'excellent',\n",
       " 'for',\n",
       " 'tearing',\n",
       " 'and',\n",
       " 'dunking',\n",
       " 'into',\n",
       " 'soup',\n",
       " '🍞',\n",
       " 'URL596']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[\"words\"][585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#me testing things\n",
    "def cap_word(word, all_letters=True):\n",
    "    if all_letters:\n",
    "        return word.upper() #all caps on the word\n",
    "    #otherwise only first letter of the word when False\n",
    "    return word.capitalize()\n",
    "\n",
    "def to_lower(word):\n",
    "    return word.lower()\n",
    "\n",
    "def irregular_capitalization(df, perc_sent, perc_words, apply_to_all=True):\n",
    "    '''\n",
    "    Input:\n",
    "    df - the training set in a pandas dataframe\n",
    "    perc_sent - the percentage of the data to be perturbed; float\n",
    "    num_words - number of words to be perturbed in a sentence; defaults to 1\n",
    "    perc_words - percentage of words to be perturbed in a sentence; used when num_words is None\n",
    "    apply_to_all - wheter to apply capitalization to all letters\n",
    "    Return: altered data + list of ids of changed sentences\n",
    "    '''\n",
    "    # random.seed(10)\n",
    "    df_copy = df.copy(deep=True)\n",
    "    num_of_sent = int(perc_sent*len(df_copy))\n",
    "    # print(f\"Number of sentences to alter: {num_of_sent}\")\n",
    "\n",
    "    #choose rand sentence ids\n",
    "    sentences_ids = random.sample(range(len(df_copy)), num_of_sent)\n",
    "    # print(\"type\", type(sentences_ids))\n",
    "    # print(\"len\", len(sentences_ids))\n",
    "    for sentence_id in sentences_ids:\n",
    "        #indexing the sentence\n",
    "        sentence = (df_copy[\"words\"][sentence_id]).copy()\n",
    "        # print(\"sentence type\", type(sentence))\n",
    "        num_words = int(perc_words*len(sentence)) #how many words to perturb\n",
    "        # print(\"num of w\", num_words)\n",
    "        if num_words > 0:\n",
    "            words_ids = random.sample(range(len(sentence)), num_words)\n",
    "            for word in words_ids:\n",
    "                if sentence[word][0].isupper():\n",
    "                    sentence[word]=sentence[word].lower()\n",
    "                else:\n",
    "                    sentence[word]=cap_word(sentence[word], all_letters=apply_to_all)\n",
    "        \n",
    "        df_copy[\"words\"][sentence_id] = sentence\n",
    "        # df[sentence_id, \"words\"] = sentence\n",
    "        # df.at[sentence_id, 'words'] = sentence\n",
    "        # df.iloc[sentence_id, 1] = sentence\n",
    "        # df.loc[sentence_id, \"words\"] = sentence\n",
    "\n",
    "    return num_of_sent, sentences_ids, df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_neighboring_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word  # Return the word unchanged if it has only one character\n",
    "\n",
    "    # Convert the word into a list of characters for easier manipulation\n",
    "    word_list = list(word)\n",
    "\n",
    "    # Choose a random index to swap with its neighboring letter\n",
    "    idx = random.randint(0, len(word) - 2)  # Ensure that the chosen index is not the last character\n",
    "\n",
    "    # Swap the character at the chosen index with its neighboring letter\n",
    "    word_list[idx], word_list[idx + 1] = word_list[idx + 1], word_list[idx]\n",
    "\n",
    "    # Convert the list of characters back to a string\n",
    "    return ''.join(word_list)\n",
    "\n",
    "def swap_letters_in_sentences(df, perc_sent, perc_words, apply_to_all=True):\n",
    "    # random.seed(10)\n",
    "    df_copy = df.copy(deep=True)\n",
    "    num_of_sent = int(perc_sent*len(df_copy))\n",
    "    # print(f\"Number of sentences to alter: {num_of_sent}\")\n",
    "\n",
    "    #choose rand sentence ids\n",
    "    sentences_ids = random.sample(range(len(df_copy)), num_of_sent)\n",
    "    # print(\"type\", type(sentences_ids))\n",
    "    # print(\"len\", len(sentences_ids))\n",
    "    for sentence_id in sentences_ids:\n",
    "        #indexing the sentence\n",
    "        sentence = (df_copy[\"words\"][sentence_id]).copy()\n",
    "        # print(\"sentence type\", type(sentence))\n",
    "        num_words = int(perc_words*len(sentence)) #how many words to perturb\n",
    "        # print(\"num of w\", num_words)\n",
    "        if num_words > 0:\n",
    "            words_ids = random.sample(range(len(sentence)), num_words)\n",
    "            for word in words_ids:\n",
    "                word_to_change = sentence[word]\n",
    "                sentence[word]=swap_neighboring_letters(word_to_change)\n",
    "        df_copy[\"words\"][sentence_id] = sentence\n",
    "\n",
    "    return num_of_sent, sentences_ids, df_copy\n",
    "\n",
    "# Swap 20% of sentences, swapping 20% of words in each selected sentenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet_list():\n",
    "        \n",
    "    # loops through the ascii codes of all lower case english letters\n",
    "    # and makes a list of the characters corresponding to those codes\n",
    "    return [chr(ascii_code) for ascii_code in range(ord(\"a\"), ord(\"z\")+1)]\n",
    "\n",
    "def insert_at_idx(word, letter, idx):\n",
    "\n",
    "    new_str = \"\" \n",
    "    new_str += word[:idx] + letter + word[idx:] #insert chosen letter at chosen index\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def insert_letter(word, seed=456):\n",
    "    \n",
    "    # random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    insert_at = random.randint(0, len(word)) #choose a random index in the word to insert at\n",
    "    # note: random.randint(start,end) is a closed interval so it takes the \"end\" number as well\n",
    "    \n",
    "    alph = get_alphabet_list()\n",
    "    letter = random.choice(alph) #choose a random english alphabet letter to be inserted\n",
    "    \n",
    "    print(f\"word {word} insert letter {letter} at idx {insert_at} (seed {seed})\")\n",
    "    \n",
    "    new_str = insert_at_idx(word, letter, insert_at) #insert chosen letter at chosen index\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def insert_multiple_letters(word, N, seed=456, set_seed=False, prints=False):\n",
    "    \n",
    "    if set_seed:\n",
    "        random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    alph = get_alphabet_list()\n",
    "    letters = [random.choice(alph) for i in range(N)] # choose N random letters from\n",
    "    # the english alphabet to insert at the chosen indices\n",
    "    \n",
    "    if prints:\n",
    "        print(f\"word {word} | (seed {seed})\")\n",
    "        print(f\"Letters to insert: {letters}\")\n",
    "    \n",
    "    new_str = word\n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        chosen_idx = random.randint(0, len(new_str)) # choose a random index to insert at\n",
    "        if prints:\n",
    "            print(f\"Inserting letter {letters[i]} at index {chosen_idx} of word {new_str}\")\n",
    "        new_str = insert_at_idx(new_str, letters[i], chosen_idx) # update the word with the chosen insertion\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def perturb_sentence(sent, perturb_func, perc, seed=456, set_seed=False):\n",
    "    \n",
    "    n_words = int(perc * len(sent))\n",
    "    if n_words == 0:\n",
    "        return sent\n",
    "    \n",
    "    if set_seed:\n",
    "        random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    new_sent = sent.copy()\n",
    "    \n",
    "    idxs = [x for x in random.sample(list(range(len(sent))), n_words)]\n",
    "    \n",
    "    for idx in idxs:\n",
    "        new_sent[idx] = perturb_func(new_sent[idx], n_words)\n",
    "        \n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertion_perturb(data, perturb_func, perc_sents, perc_words, prints=False, seed=456, set_seed=False):\n",
    "    \n",
    "    n_sents = int(perc_sents * data.shape[0])\n",
    "    if set_seed:\n",
    "        random.seed(seed) #set seed for reproducibility\n",
    "    \n",
    "    new_data = data.copy()\n",
    "    \n",
    "    idxs = [x for x in random.sample(list(range(data.shape[0])), n_sents)]\n",
    "    print(idxs)\n",
    "    \n",
    "    for idx in idxs:\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbing sentence idx {idx} | Original: \")\n",
    "            print(data[\"words\"][idx])\n",
    "            \n",
    "        new_sent = (perturb_sentence((new_data[\"words\"][idx]).copy(), perturb_func, perc_words)).copy()\n",
    "        new_data[\"words\"][idx] = new_sent\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbed version:\")\n",
    "            print(new_data[\"words\"][idx])\n",
    "        \n",
    "            print(data[\"words\"][idx] == new_data[\"words\"][idx])\n",
    "        \n",
    "    return len(idxs), idxs, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_sentence(sent, perc_words):\n",
    "    \n",
    "    n_words = int(len(sent) * perc_words)\n",
    "    if n_words == 0:\n",
    "        return sent\n",
    "    \n",
    "    word_idxs = [x for x in random.sample(list(range(len(sent))), n_words)]\n",
    "\n",
    "    for i in range(n_words):\n",
    "        \n",
    "        type_of_mistake = random.randint(1,3)\n",
    "        \n",
    "        word = sent[word_idxs[i]]\n",
    "        if len(word) > 1:\n",
    "            # mistake type 1: missed last letter\n",
    "            if type_of_mistake == 1:\n",
    "                word = word[:-1]\n",
    "            # mistake type 2: missed first letter\n",
    "            elif type_of_mistake == 2:\n",
    "                word = word[1:]\n",
    "            # mistake type 3: missed random middle letter\n",
    "            elif type_of_mistake == 3:\n",
    "                if len(word) >= 3:\n",
    "                    # make it so that you can't remove first or last letter\n",
    "                    # and have to remove smth in the middle\n",
    "                    del_idx = random.randint(1, len(word) - 2)\n",
    "                    word = word[:del_idx] + word[del_idx+1:]\n",
    "                \n",
    "        sent[word_idxs[i]] = word\n",
    "        \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_dataset(data, perc_sent, perc_words, prints=False):\n",
    "\n",
    "    n_sent = int(perc_sent * data.shape[0])\n",
    "    new_data = data.copy()\n",
    "    \n",
    "    idxs = [x for x in random.sample(list(range(data.shape[0])), n_sent)]\n",
    "    \n",
    "    for idx in idxs:\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbing sentence idx {idx} | Original: \")\n",
    "            print(data[\"words\"][idx])\n",
    "            \n",
    "        new_sent = (deletion_sentence((new_data[\"words\"][idx]).copy(), perc_words)).copy()\n",
    "        new_data[\"words\"][idx] = new_sent\n",
    "                    \n",
    "        if prints:\n",
    "            print(f\"Perturbed version:\")\n",
    "            print(new_data[\"words\"][idx])\n",
    "        \n",
    "            print(data[\"words\"][idx] == new_data[\"words\"][idx])\n",
    "        \n",
    "    return len(idxs), idxs, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_data(dataset, perc_sent, perc_words, apply_to_all=True):\n",
    "\n",
    "    num_cap_sent, ids_cap, df_cap = irregular_capitalization(dataset, perc_sent, perc_words, apply_to_all)\n",
    "    print(f\"Capped sent ids: {ids_cap}\")\n",
    "    print(\"###################cap done, start swapping##############\")\n",
    "    num_swapped_sent, ids_swapped, df_swapped = swap_letters_in_sentences(df_cap, perc_sent, perc_words, apply_to_all)\n",
    "    print(f\"Swapped sent ids: {ids_swapped}\")\n",
    "    print()\n",
    "    print(\"#########swapping done, start insertion##################\")\n",
    "    num_ins, ids_ins, df_ins = insertion_perturb(df_swapped, insert_multiple_letters, perc_sent, perc_words, prints=False)\n",
    "    print(\"#########insertion done, start deletion##################\")\n",
    "    num_del, ids_del, df_del = deletion_dataset(df_ins, perc_sent, perc_words)\n",
    "    print()\n",
    "    print(\"perturbations done, start merging lists\")\n",
    "    ids_cap.extend(ids_swapped)\n",
    "    ids_cap.extend(ids_ins)\n",
    "    ids_cap.extend(ids_del)\n",
    "    print(\"final df\")\n",
    "    final_data = df_del.copy()\n",
    "\n",
    "    return ids_cap, final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capped sent ids: [670, 635, 332, 266, 85, 333, 119]\n",
      "###################cap done, start swapping##############\n",
      "Swapped sent ids: [636, 628, 548, 451, 329, 353, 666]\n",
      "\n",
      "#########swapping done, start insertion##################\n",
      "[106, 382, 638, 373, 378, 677, 300]\n",
      "#########insertion done, start deletion##################\n",
      "\n",
      "perturbations done, start merging lists\n",
      "final df\n"
     ]
    }
   ],
   "source": [
    "ids_of_mod_sentences, new_perturbed_dev_data = perturb_data(dev_data, 0.01, 0.4)\n",
    "# ids_of_mod_sentences.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 45, 76, 85, 106, 107, 119, 266, 296, 300, 329, 332, 333, 353, 368, 373, 378, 382, 451, 548, 576, 628, 635, 636, 638, 666, 670, 677]\n"
     ]
    }
   ],
   "source": [
    "ids_of_mod_sentences.sort()\n",
    "print(ids_of_mod_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_of_mod_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "['RT', '@USER1317', ':', '\"', 'The', 'EU', 'tackles', 'climate', 'change', '\"', 'by', 'gnoring', 'th', 'building', 'f', 'coal', 'fired', 'power', 'stations', 'in', 'Germany', 'and', 'puts', 'VAT', 'on', 'soar', 'p', '…']\n",
      "['RT', '@USER1317', ':', '\"', 'The', 'EU', 'tackles', 'climate', 'change', '\"', 'by', 'ignoring', 'the', 'building', 'of', 'coal', 'fired', 'power', 'stations', 'in', 'Germany', 'and', 'puts', 'VAT', 'on', 'solar', 'p', '…']\n",
      "###########################\n",
      "['USER1095', 'Al', 'the', 'more', 'reason', 'o', 'mov', 'to', 'CA', '!', 'Humidity', 'i', 'virtually', 'nonexistent', '!']\n",
      "['@USER1095', 'All', 'the', 'more', 'reason', 'to', 'move', 'to', 'CA', '!', 'Humidity', 'is', 'virtually', 'nonexistent', '!']\n",
      "###########################\n",
      "['@USER2149', 'Whe', 'you', 'going', 'nex', '?', ':)', 'Xxx']\n",
      "['@USER2149', 'When', 'you', 'going', 'next', '?', ':)', 'Xxxx']\n",
      "###########################\n",
      "['Big', 'East', 'ceos', 'ok', 'commish', 'to', 'PURSUE', 'expansion', ':', 'The', 'presidents', 'and', 'CHANCELLORS', 'OF', 'the', '14', 'remaining', 'big', 'East', 'MEMBER', '...', 'URL141']\n",
      "['Big', 'East', 'CEOs', 'OK', 'commish', 'to', 'pursue', 'expansion', ':', 'The', 'presidents', 'and', 'chancellors', 'of', 'the', '14', 'remaining', 'Big', 'East', 'member', '...', 'URL141']\n",
      "###########################\n",
      "['Hey', 'sexy', 'love', 'm...nw', 'how', 'awrzje', 'u', 'anl?']\n",
      "['Hey', 'sexy', 'love', '...', 'how', 'are', 'u', '?']\n",
      "###########################\n",
      "['Hw', 'I', 'escribe', 'he', 'perfect', 'couple', ':', 'Actor', ',', 'Tall', ',', 'Singer', ',', 'Awkward', ',', 'Stresand', 'Worshipe', ',', 'Canadian', ',', 'ew', 'Yorker', ',', 'rummer', ',', 'Person']\n",
      "['How', 'I', 'describe', 'the', 'perfect', 'couple', ':', 'Actor', ',', 'Tall', ',', 'Singer', ',', 'Awkward', ',', 'Streisand', 'Worshiper', ',', 'Canadian', ',', 'New', 'Yorker', ',', 'Drummer', ',', 'Person']\n",
      "###########################\n",
      "['Iight', 'SO', 'how', \"'S\", 'every', 'one']\n",
      "['Iight', 'so', 'how', \"'s\", 'every', 'one']\n",
      "###########################\n",
      "['All', 'i', 'NEED', 'in', 'life', 'ARE', 'an', 'm', ',', '2', 'a', \"'S\", ',', '2', 'K', \"'s\", 'and', 'an', '1', 'R']\n",
      "['All', 'I', 'need', 'in', 'life', 'are', 'an', 'M', ',', '2', 'A', \"'s\", ',', '2', 'K', \"'s\", 'and', 'an', '1', 'R']\n",
      "###########################\n",
      "['RT', 'USER2098', ':', '0', ')', 'SO', 'CUTE', 'URL1249']\n",
      "['RT', '@USER2098', ':', '60', ')', 'SO', 'CUTE', 'URL1249']\n",
      "###########################\n",
      "['Delighted', 'to', 'aitinv', 'last', 'ceseved', 'leadership', 'frromwqm', 'our', 'Muslim', 'friends', '.', 'URnqL2a6i4']\n",
      "['Delighted', 'to', 'at', 'last', 'see', 'leadership', 'from', 'our', 'Muslim', 'friends', '.', 'URL264']\n",
      "###########################\n",
      "['@USER783', 'itv', 'palyer', '?']\n",
      "['@USER783', 'itv', 'player', '?']\n",
      "###########################\n",
      "['RT', '@USER414', ':', '#WWEPAYBACK', 'about', 'to', 'blow', 'Chicago', 'UP']\n",
      "['RT', '@USER414', ':', '#wwepayback', 'about', 'to', 'blow', 'Chicago', 'up']\n",
      "###########################\n",
      "['RT', '@USER212', ':', 'Save', 'the', 'date', '!', 'the', 'CJUSD', 'district', 'College', 'and', 'career', 'Fair', 'IS', 'Tuesday', ',', 'Oct.', '4', 'AT', 'the', 'Gonzales', 'community', 'Center', '!', 'url220', '…']\n",
      "['RT', '@USER212', ':', 'Save', 'the', 'Date', '!', 'The', 'CJUSD', 'District', 'College', 'and', 'Career', 'Fair', 'is', 'Tuesday', ',', 'Oct.', '4', 'at', 'the', 'Gonzales', 'Community', 'Center', '!', 'URL220', '…']\n",
      "###########################\n",
      "['RT', '@USER329', ':', '@USE2R572', 'hTis', 'is', 'a', 'good', 'Tweet', '.']\n",
      "['RT', '@USER329', ':', '@USER2572', 'This', 'is', 'a', 'good', 'Tweet', '.']\n",
      "###########################\n",
      "['T', '@USER22', ':', 'Iconic', 'VOTE', 'NOW', '5SOS', 'FAM', '#5SOSFa', '#iHeartAwards', 'BestFanArmy', 'URL42']\n",
      "['RT', '@USER22', ':', 'Iconic', 'VOTE', 'NOW', '5SOS', 'FAM', '#5SOSFam', '#iHeartAwards', '#BestFanArmy', 'URL426']\n",
      "###########################\n",
      "['rPrromdpboosal', 'To', 'nDiimdztchm', 'The', 'Caucus', 'Frorvaluu', 'iqfAjqn', 'Prinqmakjaray', 'Eqapwprnscs', 'Colorado', 'Political', 'Support', '–', 'KUNC', 'URL1432']\n",
      "['Proposal', 'To', 'Ditch', 'The', 'Caucus', 'For', 'A', 'Primary', 'Earns', 'Colorado', 'Political', 'Support', '–', 'KUNC', 'URL1432']\n",
      "###########################\n",
      "['icA', 'Thousand', 'Drunmsk', 'Uncles', 'URL753']\n",
      "['A', 'Thousand', 'Drunk', 'Uncles', 'URL753']\n",
      "###########################\n",
      "['RT', '@USER1650', 'e:fpnlcs', '#CaictLencmovcers', '#DonneInArte', 'oqqbWowmanwc', 'with', 'vmnvjiah', 'Cat', ',', 'Louis', 'Valtat', 'r@UtSrqiEtR52i9', 'nlm@USbER16i3n6f', '@USER1588', '@USER196', '@UlSEherR53bh7l', 'URL217', '…']\n",
      "['RT', '@USER1650', ':', '#CatLovers', '#DonneInArte', 'Woman', 'with', 'a', 'Cat', ',', 'Louis', 'Valtat', '@USER529', '@USER1636', '@USER1588', '@USER196', '@USER537', 'URL217', '…']\n",
      "###########################\n",
      "['@USER2118', '@USE1R941', 'hhaa', 'idk', 'just', 'passing', 'by', 'nad', 'saw', 'that', 'adn', \"lol'd\", '.', 'Hwo', 'si', 'it', '\"', 'ufnried', '\"', '?']\n",
      "['@USER2118', '@USER1941', 'haha', 'idk', 'just', 'passing', 'by', 'and', 'saw', 'that', 'and', \"lol'd\", '.', 'How', 'is', 'it', '\"', 'unfried', '\"', '?']\n",
      "###########################\n",
      "['omg', 'ca', \"'nt\", 'do', 'ti', 'just', 'nko', 'these', 'messages', 'speak', 'for', 'they', 'eslf', '😂']\n",
      "['omg', 'ca', \"n't\", 'do', 'it', 'just', 'kno', 'these', 'messages', 'speak', 'for', 'they', 'self', '😂']\n",
      "###########################\n",
      "['EU', 'referendum', ':', 'Ho', 'righ', 'or', 'wrong', 'ere', 'the', 'olls', '?', ':', 'URL776']\n",
      "['EU', 'referendum', ':', 'How', 'right', 'or', 'wrong', 'were', 'the', 'polls', '?', ':', 'URL776']\n",
      "###########################\n",
      "['U@SER987', '@USER50', 'Sohuld', 'Newsx', 'giev', 'coverage', 'for', 'suhc', 'cheap', 'statement', '?']\n",
      "['@USER987', '@USER50', 'Should', 'Newsx', 'give', 'coverage', 'for', 'such', 'cheap', 'statement', '?']\n",
      "###########################\n",
      "['rt', '@USER1841', ':', 'Looking', 'to', 'grow', 'YOUR', 'business', 'through', '#franchising', '?', 'Join', 'us', 'AT', 'eCom', \"'S\", 'breakfast', 'ON', '29', 'june', 'in', 'Glasgow', 'URL377', '…']\n",
      "['RT', '@USER1841', ':', 'Looking', 'to', 'grow', 'your', 'business', 'through', '#franchising', '?', 'Join', 'us', 'at', 'eCom', \"'s\", 'Breakfast', 'on', '29', 'June', 'in', 'Glasgow', 'URL377', '…']\n",
      "###########################\n",
      "['@USER1395', 'agreed']\n",
      "['@USER1395', 'agreed']\n",
      "SAME!\n",
      "###########################\n",
      "['@USER2341', 'jMoxkfcbvxqerq', 'mhxlvtzbbvujouo', 'ghbbukmsbsoq!', 'Hfbnnoqpefgpulllveyhs', 'we', 'both', 'can', 'fabxlibprngipsrahz', 'msfvqomgewqqybko', 'this', 'igaysceeaprmgvui', 'pm!mqxlhmochu', 'I', 'rnhywapvnitbcpms', 'to', 'sxvvweaioetawrtlx', 'new', 'zwsjejudhrliwecesm', ',', 'but', 'zghluIkrdbgjc', 'feel', 'bad', 'having', 'not', 'finished', 'so', 'many', '.']\n",
      "['@USER2341', 'Me', 'too', '!', 'Hopefully', 'we', 'both', 'can', 'finish', 'some', 'this', 'year', '!', 'I', 'want', 'to', 'start', 'new', 'series', ',', 'but', 'I', 'feel', 'bad', 'having', 'not', 'finished', 'so', 'many', '.']\n",
      "###########################\n",
      "['BYU', '215', 'PILLS', 'OF', 'SILDENAFIL', 'CITARTE', 'SOFT', 'TASB', '+', '52', 'BONUS', 'PILLS', ',', 'FREE', 'SHIPPING', '!', 'CLICK', 'URL4111', 'URL1074']\n",
      "['BUY', '125', 'PILLS', 'OF', 'SILDENAFIL', 'CITRATE', 'SOFT', 'TABS', '+', '25', 'BONUS', 'PILLS', ',', 'FREE', 'SHIPPING', '!', 'CLICK', 'URL1411', 'URL1074']\n",
      "###########################\n",
      "['@USER2655', 'HAVE', 'fun', 'BABYCAKE', '💘']\n",
      "['@USER2655', 'have', 'fun', 'babycake', '💘']\n",
      "###########################\n",
      "['RT', 'h@pUSEaR824', ':', 'Hope', 'everyone', 'arkis', 'goooooovdwb', '😎']\n",
      "['RT', '@USER824', ':', 'Hope', 'everyone', 'is', 'gooooood', '😎']\n"
     ]
    }
   ],
   "source": [
    "for i in ids_of_mod_sentences:\n",
    "    print(\"###########################\")\n",
    "    print(new_perturbed_dev_data[\"words\"][i])\n",
    "    print(dev_data[\"words\"][i])\n",
    "    if new_perturbed_dev_data[\"words\"][i]==dev_data[\"words\"][i]:\n",
    "        print(\"SAME!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True # dw about it\n",
    "def tokenize_and_align_labels(dataset, word_column, tag_column, tokenizer):\n",
    "    '''\n",
    "    Function tokenizes sentences and aligns the subword tokens with the labels\n",
    "    '''\n",
    "    tokenized_inputs = tokenizer(dataset[word_column].tolist(), truncation=True, is_split_into_words=True, padding = True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset[tag_column]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_Len = np.min([len(list_) for list_ in train_data[\"words\"]])\n",
    "min_Len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data and align the labels. We do this because of the subwords tokenization to get a label per token instead of per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = tokenize_and_align_labels(train_data, \"words\", \"tag_idx\", tokenizer)\n",
    "tokenized_dev_data = tokenize_and_align_labels(dev_data, \"words\", \"tag_idx\", tokenizer)\n",
    "tokenized_test_data = tokenize_and_align_labels(test_data, \"words\", \"tag_idx\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
