{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2bbbb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "807b7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"C:/Users/lilir/Desktop/natural language processing/project/project/train.bio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "aabf1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_file(path):\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['words', 'tags'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'words', 'tags']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "225482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_file(path):\n",
    "    \n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[2:]\n",
    "        \n",
    "    for line in lines:\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if line: # if line is not an empty line\n",
    "            tok = line.split('\\t')\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[3])\n",
    "            \n",
    "        else:\n",
    "            if current_words:\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            \n",
    "            \n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['words', 'tags'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'words', 'tags']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "94a6ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_bio_file(PATH_TRAIN)\n",
    "sentences = data[\"words\"]\n",
    "total_num_of_sentences = len(sentences)\n",
    "#sentences.head()\n",
    "#sentences[1]\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a1d7821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['o', 'orse', 'head', 'icking', 'ourself', 'et', 'pgrade', 'iss'] [1382, 1501, 919, 25, 742, 1567, 1318, 639] [8, 10, 7, 6, 16, 8, 30, 4]\n"
     ]
    }
   ],
   "source": [
    "def deletion(data, percent_mod_sentences):\n",
    "    #number of sentences modified (percentage-wise)\n",
    "    num_mod_sent = (percent_mod_sentences*total_num_of_sentences)//100\n",
    "\n",
    "    #random decision of the type of mistake made\n",
    "    sentences_list = list(sentences)\n",
    "    mod_wordlist = []                   # list of modified words \n",
    "    mod_sent_ids = []                   # list of modified sentence ids\n",
    "    mod_word_id_in_sent = []            # list of ids of modified words inside the sentences \n",
    "\n",
    "    type_of_mistake = random.randint(1, 3)\n",
    "    print(type_of_mistake)\n",
    "\n",
    "    #mistake type 1: missed last letter\n",
    "    if type_of_mistake == 1:\n",
    "        while len(mod_wordlist) < num_mod_sent:\n",
    "            rsent = random.choice(sentences)\n",
    "            rwordIdx = random.randint(0, len(rsent) - 1)\n",
    "            \n",
    "            rword = rsent[rwordIdx]\n",
    "            if len(rword) > 1:\n",
    "                mod_wordlist.append(rword[:-1])\n",
    "                mod_word_id_in_sent.append(rwordIdx)\n",
    "                mod_sent_ids.append(sentences_list.index(rsent))\n",
    "    \n",
    "    #mistake type 2: missed first letter\n",
    "    elif type_of_mistake == 2:    \n",
    "        while len(mod_wordlist) < num_mod_sent:\n",
    "            rsent = random.choice(sentences)\n",
    "            rwordIdx = random.randint(0, len(rsent) - 1)\n",
    "            rword = rsent[rwordIdx]\n",
    "            if len(rword) > 1:\n",
    "                mod_wordlist.append(rword[1:])\n",
    "                mod_word_id_in_sent.append(rwordIdx)\n",
    "                mod_sent_ids.append(sentences_list.index(rsent))\n",
    "\n",
    "\n",
    "    #mistake type 3: missed random letter\n",
    "    elif type_of_mistake == 3:\n",
    "        while len(mod_wordlist) < num_mod_sent:\n",
    "            rsent = random.choice(sentences)\n",
    "            rwordIdx = random.randint(0, len(rsent) - 1)\n",
    "            rword = rsent[rwordIdx]\n",
    "            if len(rword) >= 3:\n",
    "                rIdx = random.randint(0, len(rword) - 1)\n",
    "                modWord = rword[:rIdx] + rword[rIdx+1:]\n",
    "                mod_wordlist.append(modWord)\n",
    "                mod_sent_ids.append(sentences_list.index(rsent))\n",
    "                mod_word_id_in_sent.append(rwordIdx)\n",
    "\n",
    "    #returns a list of modified words, the ids of the modified sentences, the position of the modified word in its respective sentence\n",
    "    return mod_wordlist, mod_sent_ids, mod_word_id_in_sent\n",
    "\n",
    "#test\n",
    "typos, sentence_idx, words_sent_ids = deletion(sentences, 0.5)\n",
    "print(typos, sentence_idx, words_sent_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "69fedc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to'"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['words'][1382][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "4e4840d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function that overwrites the dataframe with the sentences, which contain typos\n",
    "def modify_df(dataframe, modified_sentence_ids, modified_word_idx, typos):\n",
    "    for i in range(len(typos)):\n",
    "        dataframe['words'][modified_sentence_ids[i]][modified_word_idx[i]] = typos[i]\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# test\n",
    "modified_df = modify_df(data, sentence_idx, words_sent_ids, typos)\n",
    "modified_df['words'][1382][8]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
