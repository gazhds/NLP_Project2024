{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"train.bio\"\n",
    "PATH_DEV = \"dev.bio\"\n",
    "PATH_TEST = \"test.bio\"\n",
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_file(path):\n",
    "    \n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[2:]\n",
    "        \n",
    "    for line in lines:\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if line: # if line is not an empty line\n",
    "            tok = line.split('\\t')\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[3])\n",
    "            \n",
    "        else:\n",
    "            if current_words:\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "            \n",
    "            \n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['words', 'tags'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'words', 'tags']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEOâ€™s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              words  \\\n",
       "0   0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1   1  [#Volunteers, are, key, members, of, #CHEOâ€™s, ...   \n",
       "2   2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3   3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4   4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "\n",
       "                                                tags  \n",
       "0  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3                        [O, O, O, O, O, O, O, O, O]  \n",
       "4   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = read_bio_file(PATH_TRAIN)\n",
    "dev_data = read_bio_file(PATH_DEV)\n",
    "test_data = read_bio_file(PATH_TEST)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting tag => index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-ORG': 1, 'B-PER': 2, 'B-LOC': 3, 'I-PER': 4, 'B-MISC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word is None or word == self.pad_unk:\n",
    "            return None\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                idx = len(self.idx2word)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word.append(word)\n",
    "                return idx\n",
    "            else:\n",
    "                return None\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "label_indices = Vocab()\n",
    "tags_column = train_data[\"tags\"]\n",
    "\n",
    "for tags in tags_column:\n",
    "    for tag in tags:\n",
    "        label_indices.getIdx(tag, add=True)\n",
    "\n",
    "print(label_indices.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we map the tags to indices and add them as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbc424005c9413497cac61ac7d134d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b6bbd08e2e46cf872ea735f8d70137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ffe1d125c74a149c053afddc4b4156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f8b55968a64c4cb68a123e230aec12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEOâ€™s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              words  \\\n",
       "0   0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1   1  [#Volunteers, are, key, members, of, #CHEOâ€™s, ...   \n",
       "2   2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3   3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4   4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                        [O, O, O, O, O, O, O, O, O]   \n",
       "4   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                             tag_idx  \n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tag_idx'] = train_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "dev_data['tag_idx'] = dev_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "test_data['tag_idx'] = test_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, padding=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbations\n",
    "Functions for perturbation of the dataset. We make them before tokenization for a reason. Should not change the length of the sentence (tags should still correspond)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_word(word):\n",
    "    \n",
    "    return bool(re.search('[a-zA-Z]', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_sentence(sent, word_ids):\n",
    "    \n",
    "    #n_words = len(word_ids)\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        \n",
    "        type_of_mistake = random.randint(1,3)\n",
    "        \n",
    "        word = sent[word_id]\n",
    "        if len(word) > 1:\n",
    "            # mistake type 1: missed last letter\n",
    "            if type_of_mistake == 1:\n",
    "                word = word[:-1]\n",
    "            # mistake type 2: missed first letter\n",
    "            elif type_of_mistake == 2:\n",
    "                word = word[1:]\n",
    "            # mistake type 3: missed random middle letter\n",
    "            elif type_of_mistake == 3:\n",
    "                if len(word) >= 3:\n",
    "                    # make it so that you can't remove first or last letter\n",
    "                    # and have to remove smth in the middle\n",
    "                    del_idx = random.randint(1, len(word) - 2)\n",
    "                    word = word[:del_idx] + word[del_idx+1:]\n",
    "                \n",
    "        sent[word_id] = word\n",
    "        \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_dataset(data, sent_ids, sent_words, prints=False):\n",
    "\n",
    "    new_data = (data.copy()).iloc[sent_ids]\n",
    "        \n",
    "    for idx in sent_ids:\n",
    "        \n",
    "        word_ids = sent_words[idx]\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbing sentence idx {idx} | Original: \")\n",
    "            print(data[\"words\"][idx])\n",
    "            \n",
    "        new_sent = (deletion_sentence((new_data[\"words\"][idx]).copy(), word_ids)).copy()\n",
    "        new_data[\"words\"][idx] = new_sent\n",
    "                    \n",
    "        if prints:\n",
    "            print(f\"Perturbed version:\")\n",
    "            print(new_data[\"words\"][idx])\n",
    "        \n",
    "            print(data[\"words\"][idx] == new_data[\"words\"][idx])\n",
    "        \n",
    "    return len(sent_ids), sent_ids, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet_list():\n",
    "        \n",
    "    # loops through the ascii codes of all lower case english letters\n",
    "    # and makes a list of the characters corresponding to those codes\n",
    "    return [chr(ascii_code) for ascii_code in range(ord(\"a\"), ord(\"z\")+1)]\n",
    "\n",
    "def insert_at_idx(word, letter, idx):\n",
    "\n",
    "    new_str = \"\" \n",
    "    new_str += word[:idx] + letter + word[idx:] #insert chosen letter at chosen index\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def insert_letter(word):\n",
    "        \n",
    "    insert_at = random.randint(0, len(word)) #choose a random index in the word to insert at\n",
    "    # note: random.randint(start,end) is a closed interval so it takes the \"end\" number as well\n",
    "    \n",
    "    alph = get_alphabet_list()\n",
    "    letter = random.choice(alph) #choose a random english alphabet letter to be inserted\n",
    "    \n",
    "    print(f\"word {word} insert letter {letter} at idx {insert_at}\")\n",
    "    \n",
    "    new_str = insert_at_idx(word, letter, insert_at) #insert chosen letter at chosen index\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def insert_multiple_letters(word, N=1, prints=False):\n",
    "    \n",
    "    alph = get_alphabet_list()\n",
    "    letters = [random.choice(alph) for i in range(N)] # choose N random letters from\n",
    "    # the english alphabet to insert at the chosen indices\n",
    "    \n",
    "    if prints:\n",
    "        print(f\"word {word}\")\n",
    "        print(f\"Letters to insert: {letters}\")\n",
    "    \n",
    "    new_str = word\n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        chosen_idx = random.randint(0, len(new_str)) # choose a random index to insert at\n",
    "        if prints:\n",
    "            print(f\"Inserting letter {letters[i]} at index {chosen_idx} of word {new_str}\")\n",
    "        new_str = insert_at_idx(new_str, letters[i], chosen_idx) # update the word with the chosen insertion\n",
    "    \n",
    "    return new_str\n",
    "\n",
    "def perturb_sentence(sent, word_ids, n_letters):\n",
    "    \n",
    "    n_words = len(word_ids)\n",
    "    if n_words == 0:\n",
    "        return sent\n",
    "    \n",
    "    new_sent = sent.copy()\n",
    "    \n",
    "    for idx in word_ids:\n",
    "        new_sent[idx] = insert_multiple_letters(new_sent[idx], n_letters)\n",
    "        \n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertion_perturb(data, sent_ids, sent_words, n_letters=1, prints=False):\n",
    "    \n",
    "    n_sents = len(sent_ids)\n",
    "    \n",
    "    new_data = (data.copy()).iloc[sent_ids]\n",
    "    \n",
    "    for idx in sent_ids:\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbing sentence idx {idx} | Original: \")\n",
    "            print(data[\"words\"][idx])\n",
    "         \n",
    "        word_ids = sent_words[idx]\n",
    "        new_sent = (perturb_sentence((new_data[\"words\"][idx]).copy(), word_ids, n_letters)).copy()\n",
    "        new_data[\"words\"][idx] = new_sent\n",
    "        \n",
    "        if prints:\n",
    "            print(f\"Perturbed version:\")\n",
    "            print(new_data[\"words\"][idx])\n",
    "        \n",
    "            print(data[\"words\"][idx] == new_data[\"words\"][idx])\n",
    "        \n",
    "    return len(sent_ids), sent_ids, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_neighboring_letters(word):\n",
    "    \n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "\n",
    "    # Convert the word into a list of characters for easier manipulation\n",
    "    word_list = list(word)\n",
    "\n",
    "    # Choose a random index to swap with its neighboring letter\n",
    "    idx = random.randint(0, len(word) - 2)  # Ensure that the chosen index is not the last character\n",
    "\n",
    "    # Swap the character at the chosen index with its neighboring letter\n",
    "    word_list[idx], word_list[idx + 1] = word_list[idx + 1], word_list[idx]\n",
    "\n",
    "    # Convert the list of characters back to a string\n",
    "    return ''.join(word_list)\n",
    "\n",
    "def swap_letters_in_sentences(df, sent_ids, sent_words, apply_to_all=True):\n",
    "    \n",
    "    df_copy = (df.copy()).iloc[sent_ids]\n",
    "    num_of_sent = len(sent_ids)\n",
    "\n",
    "    for sent_id in sent_ids:\n",
    "        #indexing the sentence\n",
    "        sentence = (df_copy[\"words\"][sent_id]).copy()\n",
    "        word_ids = sent_words[sent_id]\n",
    "        \n",
    "        if len(word_ids) > 0:\n",
    "            \n",
    "            for word in word_ids:\n",
    "                word_to_change = sentence[word]\n",
    "                sentence[word] = swap_neighboring_letters(word_to_change)\n",
    "                \n",
    "        df_copy[\"words\"][sent_id] = sentence\n",
    "\n",
    "    return num_of_sent, sent_ids, df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_data(dataset, perc_sent, perc_words, n_letters_insert, apply_to_all=True):\n",
    "\n",
    "    random.seed(21)\n",
    "    n_sent = math.ceil(perc_sent * dataset.shape[0])\n",
    "    sent_ids = [x for x in random.sample(list(range(dataset.shape[0])), n_sent)]\n",
    "    sent_words = {sent_id:None for sent_id in sent_ids}\n",
    "    \n",
    "    for sent_id in sent_ids:\n",
    "        sent = dataset[\"words\"][sent_id]\n",
    "        n_words = math.ceil(len(sent) * perc_words)\n",
    "        word_ids = []\n",
    "        for i in range(n_words):\n",
    "            word_id = random.randint(0,len(sent)-1)\n",
    "            word = sent[word_id]\n",
    "            while not is_valid_word(word):\n",
    "                word_id = random.randint(0,len(sent)-1)\n",
    "                word = sent[word_id]\n",
    "\n",
    "            word_ids.append(word_id)\n",
    "        sent_words[sent_id] = word_ids\n",
    "    \n",
    "    sent_ids_swap = sent_ids[:(len(sent_ids)//3)]\n",
    "    sent_ids_insert = sent_ids[(len(sent_ids)//3):((len(sent_ids)//3)*2)]\n",
    "    sent_ids_del = sent_ids[((len(sent_ids)//3)*2):]\n",
    "    \n",
    "    print(sent_ids)\n",
    "    print(len(sent_ids_swap))\n",
    "    print(len(sent_ids_insert))\n",
    "    print(len(sent_ids_del))\n",
    "    \n",
    "    num_swap, ids_swap, df_swap = swap_letters_in_sentences(dataset, sent_ids_swap, sent_words, apply_to_all)\n",
    "    num_ins, ids_ins, df_ins = insertion_perturb(dataset, sent_ids_insert, sent_words, n_letters_insert)\n",
    "    num_del, ids_del, df_del = deletion_dataset(dataset, sent_ids_del, sent_words)\n",
    "\n",
    "    merged_df = pd.concat([dataset, df_del, df_ins, df_swap]).reset_index(drop=True)\n",
    "    merged_df[\"id\"] = list(range(merged_df.shape[0]))\n",
    "    merged_df\n",
    "\n",
    "    return sent_words, merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168, 428, 706, 650, 288, 490, 221, 486, 524, 187, 517, 540, 241, 3, 14, 379, 599, 438, 70, 148, 237, 238, 43, 447, 416, 630, 452, 34, 336, 553, 509, 118, 664, 377, 23, 158]\n",
      "12\n",
      "12\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER333, :, Never, give, up, on, somethi...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[RT, @USER1300, :, The, deal, is, very, simple...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[RT, @USER2184, :, school, staff, be, like, \",...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Put, some, respeck, on, my, name]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[RT, @USER1317, :, \", The, EU, tackles, climat...</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>741</td>\n",
       "      <td>[RT, @USE1R265, :, [, PICTURE, ], @USER859, co...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>742</td>\n",
       "      <td>[RT, @USER206, :, value, people, who, see, you...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>743</td>\n",
       "      <td>[Sport, Scouting, the, Bruins, -, Lightning, E...</td>\n",
       "      <td>[O, O, O, B-ORG, O, B-ORG, O, O, O, O, B-ORG, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>744</td>\n",
       "      <td>[This, ., Tshi, exactly, ., ðŸ˜‚ðŸ˜­, URL425]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>745</td>\n",
       "      <td>[RT, @USER1430, :, Just, watched, the, last, e...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>746 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              words  \\\n",
       "0      0  [RT, @USER333, :, Never, give, up, on, somethi...   \n",
       "1      1  [RT, @USER1300, :, The, deal, is, very, simple...   \n",
       "2      2  [RT, @USER2184, :, school, staff, be, like, \",...   \n",
       "3      3                 [Put, some, respeck, on, my, name]   \n",
       "4      4  [RT, @USER1317, :, \", The, EU, tackles, climat...   \n",
       "..   ...                                                ...   \n",
       "741  741  [RT, @USE1R265, :, [, PICTURE, ], @USER859, co...   \n",
       "742  742  [RT, @USER206, :, value, people, who, see, you...   \n",
       "743  743  [Sport, Scouting, the, Bruins, -, Lightning, E...   \n",
       "744  744            [This, ., Tshi, exactly, ., ðŸ˜‚ðŸ˜­, URL425]   \n",
       "745  745  [RT, @USER1430, :, Just, watched, the, last, e...   \n",
       "\n",
       "                                                  tags  \\\n",
       "0    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                                   [O, O, O, O, O, O]   \n",
       "4    [O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O,...   \n",
       "..                                                 ...   \n",
       "741                     [O, O, O, O, O, O, O, O, O, O]   \n",
       "742                        [O, O, O, O, O, O, O, O, O]   \n",
       "743  [O, O, O, B-ORG, O, B-ORG, O, O, O, O, B-ORG, ...   \n",
       "744                              [O, O, O, O, O, O, O]   \n",
       "745  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                               tag_idx  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3                                   [0, 0, 0, 0, 0, 0]  \n",
       "4    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "741                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "742                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "743  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "744                              [0, 0, 0, 0, 0, 0, 0]  \n",
       "745  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[746 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_words, merged_df = perturb_data(dev_data, 0.05, 0.15, 1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lens = [len(x) for x in merged_df[\"words\"]]\n",
    "tags_lens = [len(x) for x in merged_df[\"tags\"]]\n",
    "sent_lens == tags_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PERTURBED_TEST = \"30p_test.bio\"\n",
    "\n",
    "P_SENT = 0.3\n",
    "P_WORDS = 0.15\n",
    "N_LETTERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PERTURBED_TRAIN = \"30p_train.bio\"\n",
    "\n",
    "P_SENT = 0.3\n",
    "P_WORDS = 0.15\n",
    "N_LETTERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    \n",
    "    txt = \"\"\n",
    "    \n",
    "    txt += \"-DOCSTART- -X- -X- O\\n\\n\"\n",
    "    \n",
    "    for sent_id in range(data.shape[0]):\n",
    "        \n",
    "        line = \"\"\n",
    "        n_words = len(data[\"words\"][sent_id])\n",
    "        \n",
    "        for word_id in range(n_words):\n",
    "            \n",
    "            line += data[\"words\"][sent_id][word_id] + \"\\t-\" + \"\\t-\\t\" + data[\"tags\"][sent_id][word_id] + \"\\n\"\n",
    "                \n",
    "        txt += line\n",
    "        if sent_id != data.shape[0] - 1:\n",
    "            txt += \"\\n\"\n",
    "        \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_perturbed_files(train_data, perc_sent, perc_words, n_letters,\n",
    "                          path_train):\n",
    "    \n",
    "    ids_train, perturbed_train_data = perturb_data(train_data, perc_sent, perc_words, n_letters)\n",
    "    \n",
    "    txt_train = format_data(perturbed_train_data)\n",
    "    \n",
    "    with open(path_train, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt_train)\n",
    "        \n",
    "    print(\"File writing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[337, 856, 1199, 576, 981, 442, 972, 1049, 375, 1034, 1080, 483, 6, 28, 759, 876, 141, 296, 475, 476, 86, 894, 833, 904, 69, 673, 1106, 1018, 237, 754, 47, 317, 181, 255, 40, 928, 329, 1130, 702, 821, 992, 309, 382, 76, 962, 481, 152, 404, 427, 145, 250, 1147, 975, 1144, 794, 411, 698, 299, 1097, 838, 597, 747, 924, 1026, 790, 651, 1160, 791, 544, 54, 191, 325, 1015, 5, 1006, 143, 1069, 454, 652, 230, 370, 348, 713, 680, 709, 627, 503, 211, 345, 134, 356, 878, 767, 327, 75, 1064, 494, 1178, 334, 29, 1116, 1005, 1036, 769, 506, 1114, 332, 1184, 899, 834, 989, 117, 589, 106, 528, 593, 71, 859, 509, 349, 807, 142, 692, 1143, 1159, 980, 101, 516, 285, 1196, 57, 434, 749, 315, 755, 872, 1134, 976, 630, 284, 565, 1072, 20, 783, 257, 48, 541, 667, 511, 862, 526, 830, 283, 273, 290, 194, 812, 192, 1158, 51, 915, 288, 147, 243, 1197, 259, 622, 664, 811, 1011, 449, 340, 371, 376, 773, 412, 94, 116, 809, 951, 8, 396, 1182, 885, 65, 407, 782, 580, 179, 874, 728, 238, 258, 987, 1118, 619, 321, 763, 42, 1150, 398, 159, 316, 743, 922, 302, 837, 515, 822, 562, 501, 970, 586, 100, 558, 615, 61, 520, 609, 752, 331, 529, 1169, 298, 716, 639, 538, 776, 574, 1045, 1016, 631, 1091, 333, 206, 363, 66, 1129, 1175, 430, 110, 888, 77, 1053, 1174, 641, 1103, 827, 778, 458, 517, 636, 1058, 696, 660, 722, 453, 482, 979, 525, 1000, 896, 1145, 72, 1121, 25, 49, 677, 444, 43, 252, 282, 721, 603, 836, 414, 741, 437, 466, 816, 242, 408, 394, 335, 498, 233, 646, 670, 87, 38, 491, 176, 1095, 381, 734, 1099, 221, 798, 460, 955, 647, 973, 634, 684, 748, 879, 620, 715, 839, 699, 268, 251, 492, 510, 395, 313, 629, 391, 844, 536, 1010, 1188, 165, 320, 966, 319, 400, 733, 148, 92, 566, 1012, 81, 534, 1076, 898, 384, 760, 863, 473, 960, 170, 720, 443, 210, 694, 343, 380, 656, 595, 697, 193, 267, 83, 1101, 1164, 527, 260, 157, 850, 519]\n",
      "120\n",
      "120\n",
      "121\n",
      "File writing complete!\n"
     ]
    }
   ],
   "source": [
    "write_perturbed_files(test_data, P_SENT, P_WORDS, N_LETTERS, PATH_PERTURBED_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PERTURBED_TRAIN = \"datasets/5p_train.bio\"\n",
    "PATH_PERTURBED_TEST = \"datasets/5p_test.bio\"\n",
    "train_data_perturbed = read_bio_file(PATH_PERTURBED_TRAIN)\n",
    "test_data_perturbed = read_bio_file(PATH_PERTURBED_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[RT, @USER2362, :, Farmall, Heart, Of, The, Ho...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[#Volunteers, are, key, members, of, #CHEOâ€™s, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[@USER2092, is, n't, it, funny, how, that, alw...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[RT, @USER80, :, Silence, is, better, than, li...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[I, just, spent, twenty, minutes, trying, to, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>1716</td>\n",
       "      <td>[hTe, plants, will, be, considreed, a, palm, t...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>1717</td>\n",
       "      <td>[RT, @USER1730, :, I, ca, n't, od, school, any...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>1718</td>\n",
       "      <td>[RT, @USRE789, :, Man, loses, everything, ni, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>1719</td>\n",
       "      <td>[RT, @USER799, :, @USER334, @USER1374, @USER17...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>1720</td>\n",
       "      <td>[BETBRIGHT, -, Get, a, Â£, 30, FREE, matched, e...</td>\n",
       "      <td>[B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1721 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              words  \\\n",
       "0        0  [RT, @USER2362, :, Farmall, Heart, Of, The, Ho...   \n",
       "1        1  [#Volunteers, are, key, members, of, #CHEOâ€™s, ...   \n",
       "2        2  [@USER2092, is, n't, it, funny, how, that, alw...   \n",
       "3        3  [RT, @USER80, :, Silence, is, better, than, li...   \n",
       "4        4  [I, just, spent, twenty, minutes, trying, to, ...   \n",
       "...    ...                                                ...   \n",
       "1716  1716  [hTe, plants, will, be, considreed, a, palm, t...   \n",
       "1717  1717  [RT, @USER1730, :, I, ca, n't, od, school, any...   \n",
       "1718  1718  [RT, @USRE789, :, Man, loses, everything, ni, ...   \n",
       "1719  1719  [RT, @USER799, :, @USER334, @USER1374, @USER17...   \n",
       "1720  1720  [BETBRIGHT, -, Get, a, Â£, 30, FREE, matched, e...   \n",
       "\n",
       "                                                   tags  \n",
       "0     [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2                     [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3                           [O, O, O, O, O, O, O, O, O]  \n",
       "4      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "...                                                 ...  \n",
       "1716                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "1717               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1718         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1719  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1720  [B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "\n",
       "[1721 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[@USER1812, No, ,, I, 'm, not, ., It, 's, defi...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[new, unique, backpack, !, combines, vintage, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[RT, @USER767, :, It, 's, been, 3, years, sinc...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[#openfollow, for, kpopers, just, retweet]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[RT, @USER526, :, empathy, 4, Kesha, ...., the...</td>\n",
       "      <td>[O, O, O, O, O, B-PER, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>1257</td>\n",
       "      <td>[RT, @USER333, :, Sometimes, I, feel, like, we...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>1258</td>\n",
       "      <td>[My, book, traielr, -, WORST, SINGING, EVER, !...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>1259</td>\n",
       "      <td>[I, m', not, difficult, I, 'm, just, abuot, my...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>1260</td>\n",
       "      <td>[I, liked, a, @USER1490, video, URL986, LQIUID...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>1261</td>\n",
       "      <td>[Disastorus, frits, innigs, SL, performed, ver...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1262 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              words  \\\n",
       "0        0  [@USER1812, No, ,, I, 'm, not, ., It, 's, defi...   \n",
       "1        1  [new, unique, backpack, !, combines, vintage, ...   \n",
       "2        2  [RT, @USER767, :, It, 's, been, 3, years, sinc...   \n",
       "3        3         [#openfollow, for, kpopers, just, retweet]   \n",
       "4        4  [RT, @USER526, :, empathy, 4, Kesha, ...., the...   \n",
       "...    ...                                                ...   \n",
       "1257  1257  [RT, @USER333, :, Sometimes, I, feel, like, we...   \n",
       "1258  1258  [My, book, traielr, -, WORST, SINGING, EVER, !...   \n",
       "1259  1259  [I, m', not, difficult, I, 'm, just, abuot, my...   \n",
       "1260  1260  [I, liked, a, @USER1490, video, URL986, LQIUID...   \n",
       "1261  1261  [Disastorus, frits, innigs, SL, performed, ver...   \n",
       "\n",
       "                                                   tags  \n",
       "0            [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1                  [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3                                       [O, O, O, O, O]  \n",
       "4        [O, O, O, O, O, B-PER, O, O, O, O, O, O, O, O]  \n",
       "...                                                 ...  \n",
       "1257  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1258               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1259                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1260                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1261  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...  \n",
       "\n",
       "[1262 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#making sure sentences and tags are still aligned\n",
    "sent_lens = [len(x) for x in test_data_perturbed[\"words\"]]\n",
    "tags_lens = [len(x) for x in test_data_perturbed[\"tags\"]]\n",
    "print(sent_lens == tags_lens)\n",
    "\n",
    "sent_lens = [len(x) for x in train_data_perturbed[\"words\"]]\n",
    "tags_lens = [len(x) for x in train_data_perturbed[\"tags\"]]\n",
    "print(sent_lens == tags_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                      1638\n",
       "words      [RT, @USER364, :, Donald, Trump, is, the, Clea...\n",
       "tags       [O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, B...\n",
       "tag_idx    [0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, ...\n",
       "Name: 1638, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                    1638\n",
       "words    [RT, @USER364, :, Donald, Trump, is, the, Clea...\n",
       "tags     [O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, B...\n",
       "Name: 1638, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_perturbed.iloc[1638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                    1720\n",
       "words    [BETBRIGHT, -, Get, a, Â£, 30, FREE, matched, e...\n",
       "tags     [B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O,...\n",
       "Name: 1720, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_perturbed.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05\n",
      "1.05\n",
      "1.1\n",
      "1.1\n",
      "1.2\n",
      "1.2\n",
      "1.3\n",
      "1.3\n"
     ]
    }
   ],
   "source": [
    "paths_to_train = [\"datasets/5p_train.bio\", \"datasets/10p_train.bio\", \"datasets/20p_train.bio\", \"datasets/30p_train.bio\"]\n",
    "paths_to_test = [\"datasets/5p_test.bio\", \"datasets/10p_test.bio\", \"datasets/20p_test.bio\", \"datasets/30p_test.bio\"]\n",
    "#Checking that the sizes of new datasets are a fraction bigger than OG ones\n",
    "for i in range(len(paths_to_train)):\n",
    "    traindata = read_bio_file(paths_to_train[i])\n",
    "    testdata = read_bio_file(paths_to_test[i])\n",
    "    print(np.round(len(traindata)/len(train_data), 2))\n",
    "    print(np.round(len(testdata)/len(test_data), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
