{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5af446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3927cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"C:/Users/kelis/ITU/Year_2/4th_semester/NLP/project/handout/project/en_ewt-ud-train.iob2\"\n",
    "PATH_DEV = \"C:/Users/kelis/ITU/Year_2/4th_semester/NLP/project/handout/project/en_ewt-ud-dev.iob2\"\n",
    "PATH_TEST = \"C:/Users/kelis/ITU/Year_2/4th_semester/NLP/project/handout/project/en_ewt-ud-test-masked.iob2\"\n",
    "PATH_OUTPUT = \"C:/Users/kelis/ITU/Year_2/4th_semester/NLP/project/scripts/outputs.txt\"\n",
    "UNK = \"[UNK]\"\n",
    "PAD = 0\n",
    "MLM = 'distilbert-base-cased'\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_SENTS=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595f07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, mlm: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        mlm : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.mlm = AutoModel.from_pretrained(mlm)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.mlm.config, 'hidden_size'):\n",
    "            self.mlm_out_size = self.mlm.config.hidden_size\n",
    "        elif hasattr(self.mlm.config, 'dim'):\n",
    "            self.mlm_out_size = self.mlm.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.mlm_out_size = 768\n",
    "            \n",
    "        print(f\"Hidden size: {self.mlm_out_size}\")\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.mlm_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        mlm_out = self.mlm(input)\n",
    "\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out[:,:,:].squeeze() \n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(mlm_out)\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def run_eval(self, feats_batches, labels_batches):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for sents, labels in zip(feats_batches, labels_batches):\n",
    "            output_scores = self.forward(sents)\n",
    "            predicted_tags  = torch.argmax(output_scores, 2)\n",
    "            for goldSent, predSent in zip(labels, predicted_tags):\n",
    "                for goldLabel, predLabel in zip(goldSent, predSent):\n",
    "                    if goldLabel.item() != 0:\n",
    "                        total += 1\n",
    "                        if goldLabel.item() == predLabel.item():\n",
    "                            match+= 1\n",
    "        return(match/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c273ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2lookup(labels, PAD):\n",
    "\n",
    "    id2label = [PAD, 'O', 'B-LOC', 'I-LOC', 'B-PER', 'B-ORG', 'I-ORG', 'I-PER']\n",
    "    label2id = {PAD: 0, 'O': 1, 'B-LOC': 2, 'I-LOC': 3, 'B-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'I-PER': 7}\n",
    "                \n",
    "    return id2label, label2id\n",
    "\n",
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    read in iob2 file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data_words = []\n",
    "    data_tags = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data_words.append(current_words)\n",
    "                data_tags.append(current_tags)\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data_words.append(current_words)\n",
    "        data_tags.append(current_tags)\n",
    "    return data_words, data_tags\n",
    "\n",
    "def find_max_len(data):\n",
    "    \n",
    "    max_len = max([len(x) for x in data])\n",
    "    \n",
    "    return max_len\n",
    "\n",
    "def pad_data(data, PAD, N):\n",
    "    \n",
    "    padded = []\n",
    "    \n",
    "    for sent in data:\n",
    "        \n",
    "        new_sent = sent.copy()\n",
    "        \n",
    "        sent_len = len(sent)\n",
    "        dif = N - sent_len\n",
    "        \n",
    "        for _ in range(dif):\n",
    "            \n",
    "            new_sent.append(PAD)\n",
    "            \n",
    "        padded.append(new_sent)\n",
    "        \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146b79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    train_text, train_labels = read_data(PATH_TRAIN)\n",
    "    dev_text, dev_labels = read_data(PATH_DEV)\n",
    "    test_text, test_labels = read_data(PATH_TEST)\n",
    "\n",
    "    train_text = train_text[:MAX_SENTS] \n",
    "    train_labels = train_labels[:MAX_SENTS]\n",
    "    \n",
    "    dev_text = dev_text[:MAX_SENTS]\n",
    "    dev_labels = dev_labels[:MAX_SENTS]\n",
    "    \n",
    "    test_text = test_text[:MAX_SENTS]\n",
    "    test_labels = test_labels[:MAX_SENTS]\n",
    "    \n",
    "    return train_text, train_labels, dev_text, dev_labels, test_text, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5743a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    \n",
    "    id2label, label2id = labels2lookup(train_labels, UNK)\n",
    "    \n",
    "    enc_labels = labels.copy() \n",
    "    \n",
    "    for i, label_list in enumerate(labels):\n",
    "        for j, label in enumerate(label_list):\n",
    "            enc_labels[i][j]= label2id[label]\n",
    "            \n",
    "    return enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "03d992a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(words, tags, tokenizer, label_all_tokens=True):\n",
    "    \n",
    "    toks = tokenizer(words, truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(tags):\n",
    "        \n",
    "        word_ids = toks.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else 0)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    toks[\"labels\"] = labels\n",
    "    \n",
    "    return toks.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0937a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_data, label_data):\n",
    "    \n",
    "    tokzr = AutoTokenizer.from_pretrained(MLM)\n",
    "    tok_data = tokenize_words(text_data, label_data, tokzr)\n",
    "    enc_words = tok_data[\"input_ids\"]\n",
    "    label_data = tok_data[\"labels\"]\n",
    "    \n",
    "    max_sent_len = find_max_len(enc_words)\n",
    "    print(f\"Max sent len is {max_sent_len}\")\n",
    "    \n",
    "    print(f\"Padding all sentences to {max_sent_len} tokens using pad token {PAD}\")\n",
    "    \n",
    "    padded_data = pad_data(enc_words, PAD, max_sent_len)\n",
    "    words = np.array(padded_data)\n",
    "    words = torch.tensor(words)\n",
    "    \n",
    "    print(f\"Text data converted to pytorch tensor, shape {words.shape}\")\n",
    "    \n",
    "    return words, tok_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e6d6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(label_data):\n",
    "    \n",
    "    max_sent_len = find_max_len(label_data)\n",
    "    padded_label_data = pad_data(label_data, PAD, max_sent_len)\n",
    "    labels = torch.tensor(padded_label_data)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c9c9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(text_data):\n",
    "    \n",
    "    num_batches = int(len(text_data)/BATCH_SIZE)\n",
    "    print(f\"num batches: {num_batches}\")\n",
    "    \n",
    "    max_sent_len = find_max_len(text_data)\n",
    "    batches = text_data[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_sent_len)\n",
    "    \n",
    "    print(f\"Got {num_batches} batches of size {BATCH_SIZE}, final tensor shape is {batches.shape}\")\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "266e6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_labels, dev_text, dev_labels, test_text, test_labels = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b130d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_labels_train = encode_labels(train_labels)\n",
    "enc_labels_dev = encode_labels(dev_labels)\n",
    "enc_labels_test = encode_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c80d766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sent len is 47\n",
      "Padding all sentences to 47 tokens using pad token 0\n",
      "Text data converted to pytorch tensor, shape torch.Size([64, 47])\n",
      "Max sent len is 77\n",
      "Padding all sentences to 77 tokens using pad token 0\n",
      "Text data converted to pytorch tensor, shape torch.Size([64, 77])\n",
      "Max sent len is 51\n",
      "Padding all sentences to 51 tokens using pad token 0\n",
      "Text data converted to pytorch tensor, shape torch.Size([64, 51])\n"
     ]
    }
   ],
   "source": [
    "pre_train, tok_data_train, labels_train = preprocess_text(train_text, enc_labels_train)\n",
    "pre_dev, tok_data_dev, labels_dev = preprocess_text(dev_text, enc_labels_dev)\n",
    "pre_test, tok_data_test, _ = preprocess_text(test_text, enc_labels_test)\n",
    "pre_train_labels = preprocess_labels(labels_train)\n",
    "pre_dev_labels = preprocess_labels(labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ff87f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batches: 2\n",
      "Got 2 batches of size 32, final tensor shape is torch.Size([2, 32, 47])\n",
      "num batches: 2\n",
      "Got 2 batches of size 32, final tensor shape is torch.Size([2, 32, 47])\n",
      "num batches: 2\n",
      "Got 2 batches of size 32, final tensor shape is torch.Size([2, 32, 77])\n",
      "num batches: 2\n",
      "Got 2 batches of size 32, final tensor shape is torch.Size([2, 32, 77])\n"
     ]
    }
   ],
   "source": [
    "train_batches = get_batches(pre_train)\n",
    "train_label_batches = get_batches(pre_train_labels)\n",
    "dev_batches = get_batches(pre_dev)\n",
    "dev_label_batches = get_batches(pre_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "146f103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "id2label, label2id = labels2lookup(train_labels, UNK)\n",
    "NLABELS = len(id2label)\n",
    "model = ClassModel(NLABELS, MLM)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d4b9e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "starting epoch 0\n",
      "---running batch idx 0---\n",
      "size of current batch: torch.Size([32, 47])\n",
      "output scores shape torch.Size([1504, 8])\n",
      "train labels in a single batch:  torch.Size([32, 47])\n",
      "predicted labels size: torch.Size([1504])\n",
      "---running batch idx 1---\n",
      "size of current batch: torch.Size([32, 47])\n",
      "output scores shape torch.Size([1504, 8])\n",
      "train labels in a single batch:  torch.Size([32, 47])\n",
      "predicted labels size: torch.Size([1504])\n",
      "Loss: 2242.91\n",
      "Acc(dev): 11.46\n",
      "\n",
      "=====================\n",
      "starting epoch 1\n",
      "---running batch idx 0---\n",
      "size of current batch: torch.Size([32, 47])\n",
      "output scores shape torch.Size([1504, 8])\n",
      "train labels in a single batch:  torch.Size([32, 47])\n",
      "predicted labels size: torch.Size([1504])\n",
      "---running batch idx 1---\n",
      "size of current batch: torch.Size([32, 47])\n",
      "output scores shape torch.Size([1504, 8])\n",
      "train labels in a single batch:  torch.Size([32, 47])\n",
      "predicted labels size: torch.Size([1504])\n",
      "Loss: 1988.14\n",
      "Acc(dev): 49.34\n",
      "\n",
      "=====================\n",
      "starting epoch 2\n",
      "---running batch idx 0---\n",
      "size of current batch: torch.Size([32, 47])\n",
      "output scores shape torch.Size([1504, 8])\n",
      "train labels in a single batch:  torch.Size([32, 47])\n",
      "predicted labels size: torch.Size([1504])\n",
      "---running batch idx 1---\n",
      "size of current batch: torch.Size([32, 47])\n",
      "output scores shape torch.Size([1504, 8])\n",
      "train labels in a single batch:  torch.Size([32, 47])\n",
      "predicted labels size: torch.Size([1504])\n",
      "Loss: 1750.04\n",
      "Acc(dev): 78.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = train_batches.shape[2]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "        print('=====================')\n",
    "        print('starting epoch ' + str(epoch))\n",
    "        model.train() \n",
    "    \n",
    "        # Loop over batches\n",
    "        loss = 0\n",
    "        for batch_idx in range(0, len(train_batches)):\n",
    "\n",
    "            print(f\"---running batch idx {batch_idx}---\")\n",
    "            print(f\"size of current batch: {train_batches[batch_idx].shape}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output_scores = model.forward(train_batches[batch_idx])\n",
    "            \n",
    "            flat_labels = train_label_batches[batch_idx].view(BATCH_SIZE * MAX_SENT_LEN)\n",
    "            output_scores = output_scores.view(BATCH_SIZE * MAX_SENT_LEN, -1)\n",
    "            \n",
    "            print(\"output scores shape\", output_scores.shape)\n",
    "            \n",
    "            batch_loss = loss_function(output_scores, flat_labels)\n",
    "            \n",
    "            predicted_labels = torch.argmax(output_scores, 1)\n",
    "            #predicted_labels = predicted_labels.view(BATCH_SIZE, MAX_SENT_LEN)\n",
    "\n",
    "            print(\"train labels in a single batch: \", train_label_batches[batch_idx].shape)\n",
    "            print(f\"predicted labels size: {predicted_labels.shape}\")\n",
    "\n",
    "            loss += batch_loss.item()\n",
    "    \n",
    "            batch_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        dev_score = model.run_eval(dev_batches, dev_label_batches)\n",
    "        print('Loss: {:.2f}'.format(loss))\n",
    "        print('Acc(dev): {:.2f}'.format(100*dev_score))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
